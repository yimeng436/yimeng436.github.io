<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>PyTorch | 异梦的博客</title><meta name="keywords" content="深度学习"><meta name="author" content="异梦"><meta name="copyright" content="异梦"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="PyTorch">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch">
<meta property="og:url" content="https://yimeng436.github.io/2022/06/15/PyTorch/PyTorch/index.html">
<meta property="og:site_name" content="异梦的博客">
<meta property="og:description" content="PyTorch">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yimeng436.github.io/img/top.png">
<meta property="article:published_time" content="2022-06-14T16:00:00.000Z">
<meta property="article:modified_time" content="2022-07-15T01:15:47.021Z">
<meta property="article:author" content="异梦">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yimeng436.github.io/img/top.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://yimeng436.github.io/2022/06/15/PyTorch/PyTorch/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'PyTorch',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-07-15 09:15:47'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="http://mms2.baidu.com/it/u=665033858,976373917&amp;fm=253&amp;app=138&amp;f=JPEG&amp;fmt=auto&amp;q=75?w=500&amp;h=500" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">31</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/top.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">异梦的博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">PyTorch</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-06-14T16:00:00.000Z" title="发表于 2022-06-15 00:00:00">2022-06-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-07-15T01:15:47.021Z" title="更新于 2022-07-15 09:15:47">2022-07-15</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="PyTorch"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><strong><center><font size=5>PyTorch</font></center></strong></p>
<h1 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h1><h2 id="函数创建"><a href="#函数创建" class="headerlink" title="函数创建"></a>函数创建</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">3</span>])   <span class="comment">#参数是一个序列(列表、元组、array)</span></span><br><span class="line"></span><br><span class="line">t = torch.from_numpy(np.arange(<span class="number">10</span>))   <span class="comment">#将numpy的ndarray对象转为									tensor对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#随机创建0~1,参数为形状</span></span><br><span class="line">t = torch.rand(<span class="number">2</span>,<span class="number">3</span>)			<span class="comment">#tensor([[0.6696, 0.0476, 0.5098],</span></span><br><span class="line">        					<span class="comment">#[0.9655, 0.2699, 0.4436]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#随机创建符合标准正态分布的tensor</span></span><br><span class="line">t = torch.randn(<span class="number">2</span>,<span class="number">3</span>)	<span class="comment">#tensor([[-0.5311, -0.8295,  1.4274],</span></span><br><span class="line">        				<span class="comment">#[ 1.2927, -0.4475, -0.1752]])</span></span><br><span class="line">    </span><br><span class="line">  </span><br><span class="line"><span class="comment">#全零、一，参数为形状</span></span><br><span class="line">torch.zeros(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="张量类型"><a href="#张量类型" class="headerlink" title="张量类型"></a>张量类型</h3><p>张量中每一个数据的类型，通过tensor对象的dtype属性查看</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line">t.dtype							<span class="comment">#torch.int64(默认)。用array创建									为int32。对于浮点型默认是											float32、array创建为float64</span></span><br><span class="line"></span><br><span class="line">t1 = torch.tensor([<span class="number">1</span>,<span class="number">3</span>],dtype=torch.int16)  <span class="comment">#指定类型创建</span></span><br><span class="line"></span><br><span class="line">t = torch.FloatTensor([<span class="number">1</span>,<span class="number">3</span>])		<span class="comment"># float32</span></span><br><span class="line">t = torch.LongTensor([<span class="number">1</span>,<span class="number">3</span>])			<span class="comment"># int64</span></span><br></pre></td></tr></table></figure>



<h3 id="张量类型转换"><a href="#张量类型转换" class="headerlink" title="张量类型转换"></a>张量类型转换</h3><p>最常见的是隐式转换</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1.1</span>,<span class="number">2</span>])    <span class="comment">#结果为tensor([1.1000,2.00000])</span></span><br><span class="line">							<span class="comment">#隐式转为float类型</span></span><br></pre></td></tr></table></figure>



<p>还可以通过函数进行转换</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line">t.<span class="built_in">float</span>()					<span class="comment">#结果为 tentor([1.,3.])</span></span><br><span class="line">t.double()				<span class="comment">#结果为tensor([1.,3.],dtype=float64)</span></span><br><span class="line">t.long()				<span class="comment">#torch.int64</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将维度为0的tensor对象转为python类型</span></span><br><span class="line"><span class="comment">#使用item()方法</span></span><br><span class="line"><span class="built_in">print</span>(t1)				<span class="comment">#tensor(-1, device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(t1.item())		<span class="comment">#-1</span></span><br></pre></td></tr></table></figure>



<h3 id="张量维度"><a href="#张量维度" class="headerlink" title="张量维度"></a>张量维度</h3><p>通过tensor.ndim查看张量的维度</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line">t.ndim 					<span class="comment"># 1 </span></span><br><span class="line">t.shape					<span class="comment"># tensor.size([2])</span></span><br><span class="line"><span class="built_in">len</span>(t)					<span class="comment"># 2 (1维时表示两个元素)</span></span><br><span class="line">t.numel()				<span class="comment"># 2 一共有几个元素</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t = torch.tensor([[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">4</span>]])</span><br><span class="line">t.ndim					<span class="comment"># 2</span></span><br><span class="line">t.shape					<span class="comment"># tensor.size([2,2])</span></span><br><span class="line"><span class="built_in">len</span>(t)					<span class="comment"># 2 (有两个一维张量构成)</span></span><br><span class="line">t.numel()				<span class="comment"># 4 一共4个元素</span></span><br></pre></td></tr></table></figure>



<h3 id="tensor设备"><a href="#tensor设备" class="headerlink" title="tensor设备"></a>tensor设备</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看tensor处于什么设备</span></span><br><span class="line">t = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t.device					<span class="comment">#device(type=&#x27;cpu&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看显卡是否可用，可用则转为gpu</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available()：</span><br><span class="line">	t = t.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">   </span><br><span class="line">t.device					<span class="comment">#device(type=&#x27;cuda&#x27;, index=0)</span></span><br></pre></td></tr></table></figure>



<h3 id="张量运算"><a href="#张量运算" class="headerlink" title="张量运算"></a>张量运算</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(t)					<span class="comment">#tensor([[-1,  0,  0],</span></span><br><span class="line">        					<span class="comment">#[ 0,  0,  0]], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(t1)					<span class="comment">#tensor([[1., 1., 1.],</span></span><br><span class="line">        					<span class="comment">#[1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加法运算</span></span><br><span class="line">  <span class="comment">#加上一个标量，会对每一个元素进行相加</span></span><br><span class="line">    t + <span class="number">3</span> 					<span class="comment">#tensor([[2, 3, 3],</span></span><br><span class="line">        					<span class="comment">#[3, 3, 3]], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line">  <span class="comment">#相同形状张量相加，对应位置进行相加</span></span><br><span class="line">	t+t1					<span class="comment">#tensor([[0., 1., 1.],</span></span><br><span class="line">        					<span class="comment">#[1., 1., 1.]], device=&#x27;cuda:0&#x27;)</span></span><br></pre></td></tr></table></figure>



<h1 id="张量变化"><a href="#张量变化" class="headerlink" title="张量变化"></a>张量变化</h1><h2 id="flatten-拉平"><a href="#flatten-拉平" class="headerlink" title="flatten() 拉平"></a>flatten() 拉平</h2><p>任意维度转为1维张量</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">4</span>]])</span><br><span class="line">t.flatten() 				<span class="comment">#tensor([1,3,2,4])</span></span><br></pre></td></tr></table></figure>



<h2 id="reshape-hang-lie"><a href="#reshape-hang-lie" class="headerlink" title="reshape(hang , lie)"></a>reshape(hang , lie)</h2><p>将张量转为  <strong>（hang，lie）</strong> 的形状，变换的size这个属性</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">t1 = torch.tensor([<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(t1)						<span class="comment">#tensor([1, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#两个包含一个元素的张量</span></span><br><span class="line"><span class="built_in">print</span>(t1.reshape(<span class="number">2</span>, <span class="number">1</span>))			<span class="comment">#tensor([[1],</span></span><br><span class="line">								<span class="comment">#		[3]])</span></span><br><span class="line"><span class="comment">#一个包含两个元素的张量  </span></span><br><span class="line"><span class="built_in">print</span>(t1.reshape(<span class="number">1</span>,<span class="number">2</span>))			<span class="comment">#tensor([[1, 3]])</span></span><br></pre></td></tr></table></figure>



<h2 id="squeeze-tensor"><a href="#squeeze-tensor" class="headerlink" title="squeeze(tensor)"></a>squeeze(tensor)</h2><p>参数为一个tensor对象，该方法可以删掉维度为1的tensor对象。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">6</span>)</span><br><span class="line">t.shape						<span class="comment">#torch.Size([1, 3, 6])</span></span><br><span class="line">t2 = torch.squeeze(t)</span><br><span class="line">t2.shape					<span class="comment">#torch.Size([3, 6])</span></span><br></pre></td></tr></table></figure>



<p><strong>用reshape()完成flatten()</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">res = np.array(<span class="built_in">range</span>(<span class="number">12</span>))</span><br><span class="line"></span><br><span class="line">res = res.reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">res = torch.tensor(res)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(res)				<span class="comment">#tensor([[ 0,  1,  2,  3],</span></span><br><span class="line">       					<span class="comment">#[ 4,  5,  6,  7],</span></span><br><span class="line">       					<span class="comment">#[ 8,  9, 10, 11]], 										dtype=torch.int32)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(res.flatten())	<span class="comment">#tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=torch.int32)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(res.reshape(res.numel()))	 <span class="comment">#tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=torch.int32)</span></span><br></pre></td></tr></table></figure>



<h1 id="张量索引"><a href="#张量索引" class="headerlink" title="张量索引"></a>张量索引</h1><h2 id="一维张量索引"><a href="#一维张量索引" class="headerlink" title="一维张量索引"></a>一维张量索引</h2><p>一维张量索引和普通数据的索引方式相同，用下标或者[start：end]方式。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">t1 = torch.tensor(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">10</span>)))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(t1[<span class="number">2</span>:<span class="number">5</span>])					<span class="comment">#tensor([2, 3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#[: : step],step 必须大于0</span></span><br><span class="line"><span class="built_in">print</span>(t1[::<span class="number">2</span>])					<span class="comment">#tensor([0, 2, 4, 6, 8])</span></span><br></pre></td></tr></table></figure>



<h2 id="二维张量索引"><a href="#二维张量索引" class="headerlink" title="二维张量索引"></a>二维张量索引</h2><p>二维索引和二维数组的索引相同，需要多一个索引项，**[ a , b ]**。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">t1 = torch.arange(<span class="number">0</span>,<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(t1.size())            <span class="comment">#torch.Size([3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(t1)					<span class="comment"># tensor([[0, 1, 2],</span></span><br><span class="line">							<span class="comment">#         [3, 4, 5],</span></span><br><span class="line">							<span class="comment">#         [6, 7, 8]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#符号索引：</span></span><br><span class="line"><span class="built_in">print</span>(t1[<span class="number">0</span>])				<span class="comment">#tensor([0, 1, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(t1[<span class="number">0</span>,<span class="number">1</span>])				<span class="comment">#tensor(1)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(t1[<span class="number">0</span>:<span class="number">1</span>,<span class="number">0</span>:<span class="number">1</span>])			<span class="comment">#tensor([[0]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#函数式索引：</span></span><br><span class="line">index = torch.tensor([<span class="number">1</span>,<span class="number">2</span>])	</span><br><span class="line"></span><br><span class="line"><span class="comment">#第二个参数就是表示在第几维上进行索引，取值范围就是size的长度。</span></span><br><span class="line"><span class="comment">#二维0表示行的维度上</span></span><br><span class="line"><span class="built_in">print</span>(torch.index_select(t1, <span class="number">0</span>, index))		<span class="comment">#tensor([[3, 4, 5]</span></span><br><span class="line">											<span class="comment">#	[6, 7, 8]])</span></span><br><span class="line"><span class="comment">#二维1表示列的维度上    </span></span><br><span class="line"><span class="built_in">print</span>(torch.index_select(t1, <span class="number">1</span>, index))		<span class="comment">#tensor([[1, 2],</span></span><br><span class="line">                                            <span class="comment"># 		[4, 5],</span></span><br><span class="line">                                            <span class="comment">#		[7, 8]])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="tensor-view"><a href="#tensor-view" class="headerlink" title="tensor.view()"></a>tensor.view()</h2><p>该方法会返回一个tensor对象的视图，该视图可以和原tensor对象的结构不相同，视图和原tensor对象公用同一块内存地址，类似于浅拷贝，修改其中的一个另一个也会跟着改变。在后续很多操作如分片合并得到的都是一个tensor对象的一个视图。</p>
<p><img src="/images/image-20220617222644522.png" alt="image-20220617222644522"></p>
<h1 id="张量切分与合并"><a href="#张量切分与合并" class="headerlink" title="张量切分与合并"></a>张量切分与合并</h1><h2 id="切分"><a href="#切分" class="headerlink" title="切分"></a>切分</h2><h3 id="torch-chunk-tensor，num，dim"><a href="#torch-chunk-tensor，num，dim" class="headerlink" title="torch.chunk(tensor，num，dim)"></a>torch.chunk(tensor，num，dim)</h3><p>第一个参数是一个tensor对象，第二个参数是要切分成多少份且只能均分，第三个参数是在第几个维度上进行切分。返回的是一个包含tensor对象的元组，且元组的每一个元素的维度和原tensor对象的维度相同且是一个视图。</p>
<p><img src="/images/image-20220618220106953.png" alt="image-20220618220106953"></p>
<h3 id="torch-split-tensor-dim"><a href="#torch-split-tensor-dim" class="headerlink" title="torch.split(tensor,[],dim)"></a>torch.split(tensor,[],dim)</h3><p>第一个参数是被切分的tensor对象，第二个参数可以是一个整数也可以是一个集合，是一个整数时表示均匀切分，第三个参数是在第几维度上进行切分。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">t = torch.arange(<span class="number">12</span>).reshape(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#整数</span></span><br><span class="line">ts = torch.split(t,<span class="number">2</span>,<span class="number">0</span>)	<span class="comment">#(tensor([[50,  1,  2],</span></span><br><span class="line">                        <span class="comment">#         [ 3,  4,  5]]),</span></span><br><span class="line">                        <span class="comment"># tensor([[ 6,  7,  8],</span></span><br><span class="line">                        <span class="comment">#         [ 9, 10, 11]]))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#[a,b....]  a+b+..要等于tensor对象在 dim维度上的数目</span></span><br><span class="line">ts = torch.split(t,[<span class="number">1</span>,<span class="number">3</span>],<span class="number">0</span>)    </span><br><span class="line">                                <span class="comment">#(tensor([[50,  1,  2]]),</span></span><br><span class="line">                                <span class="comment"># tensor([[ 3,  4,  5],</span></span><br><span class="line">                                <span class="comment">#         [ 6,  7,  8],</span></span><br><span class="line">                                <span class="comment">#         [ 9, 10, 11]]))</span></span><br></pre></td></tr></table></figure>





<h2 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h2><h3 id="拼接：cat-tensor1，tensor2-dim-x3D-0"><a href="#拼接：cat-tensor1，tensor2-dim-x3D-0" class="headerlink" title="拼接：cat([tensor1，tensor2],dim&#x3D;0)"></a>拼接：cat([tensor1，tensor2],dim&#x3D;0)</h3><p>第一个参数是需要合并的两个tensor对象组成的集合，第二个参数是在哪一个维度上进行合并（默认是0）。拼接时两个张量在dim维度上的数量要匹配否则报错。</p>
<p><img src="/images/image-20220618224635865.png" alt="image-20220618224635865"></p>
<h3 id="堆叠：stack-tensor1-tensor2-，dim"><a href="#堆叠：stack-tensor1-tensor2-，dim" class="headerlink" title="堆叠：stack([tensor1,tensor2]，dim)"></a>堆叠：stack([tensor1,tensor2]，dim)</h3><p>参数含义和cat相同。拼接是在指定维度上增加上一个张量，不会增加维度。而堆叠是将两个张量放在一个新的维度上进行摆放，会增加维度。两个张量的shape要相同。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a = torch.zeros(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">c = torch.stack([a,b])</span><br><span class="line">                            <span class="comment">#tensor([[[0., 0., 0.],</span></span><br><span class="line">                            <span class="comment">#         [0., 0., 0.]],</span></span><br><span class="line"></span><br><span class="line">                            <span class="comment">#        [[1., 1., 1.],</span></span><br><span class="line">                            <span class="comment">#         [1., 1., 1.]]])</span></span><br><span class="line">                </span><br><span class="line">d.shape					<span class="comment">#torch.Size([2, 2, 3])         </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.cat([a,b]).shape		<span class="comment">#torch.Size([4, 3])</span></span><br></pre></td></tr></table></figure>



<h1 id="张量的自动微分"><a href="#张量的自动微分" class="headerlink" title="张量的自动微分"></a>张量的自动微分</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#判断张量是否被pytorch跟着计算过程</span></span><br><span class="line">t.requires_grad				<span class="comment">#False表示不跟踪，True表示跟踪</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#直接改变</span></span><br><span class="line">t.requires_grad_(<span class="literal">True</span>/<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在创建张量时时可以通过指定requires_grad	属性</span></span><br><span class="line">t = torch.ones(<span class="number">2</span>,<span class="number">3</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">t.requires_grad				<span class="comment">#True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#torch中，要计算梯度一般都是用标量进行计算，用计算得到的标量调用backward,也就是调用自动微分运算</span></span><br><span class="line"></span><br><span class="line">t = t+<span class="number">5</span></span><br><span class="line">t = t*<span class="number">2</span></span><br><span class="line">out = t.mean()				<span class="comment">#tensor(12., grad_fn=										&lt;MeanBackward0&gt;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#backword调用之后会计算每一层的一个梯度，可以通过grad属性查看每一层的梯度</span></span><br><span class="line">out.backward()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#在训练过程中，有一些值不希望被跟踪如loss、acc等，可以将这部分的值的计算放在with torch.no_grad(): 这个上下文管理器里面</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y = t+<span class="number">5</span></span><br><span class="line">    <span class="built_in">print</span>(y.requires_grad)				<span class="comment">#False</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#也可以通过.detach()方法直接截断跟踪</span></span><br><span class="line">res = out.detach()</span><br><span class="line">res.require_grad						<span class="comment">#False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#梯度置零</span></span><br><span class="line">t.grad.data.zero_()</span><br><span class="line">y.grad.data.zero_()</span><br></pre></td></tr></table></figure>



<h1 id="torchvision-库"><a href="#torchvision-库" class="headerlink" title="torchvision 库"></a>torchvision 库</h1><p>这个库提供了很多初学者使用的数据集，MINNST、CIFAR….；数据集都包含在torchvision.datasets下。</p>
<p>在处理图像数据时，torch默认处理的数据格式为 ：[通道，宽，高] 的形式，在读取数据集时，数据集本身的形式是 ：[宽，高，通道] 的形式。所以训练之前需要将数据集格式进行转换，而转换主要采用的就是<strong>torchvision.transforms</strong>  模块下的 <strong>ToTensor()</strong> 方法。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"></span><br><span class="line">data = torchvision.datasets.MNIST(<span class="string">&quot;./data&quot;</span>,train=<span class="literal">True</span>,transform=ToTensor(),download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>



<p>ToTensor()方法作用：</p>
<p>1、输入转换为Tensor对象</p>
<p>2、格式转换为 [通道，宽，高] 的形式</p>
<p>3、对图片数据归一化，像素点同除255</p>
<h1 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h1><h2 id="TensorDataset类"><a href="#TensorDataset类" class="headerlink" title="TensorDataset类"></a>TensorDataset类</h2><p>在数据切分过程中可以用torch所包含的TensorDataset来简化<br>TensorDataset 实现了_ <em>getitem</em> _方法可以直接用索引进行切片</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.data <span class="keyword">import</span> TensorDataset</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据处理完之后直接将X,Y都包装成TensorDataset</span></span><br><span class="line">Data = TensorDataset(X,Y)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoches):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(num_of_batch):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#对包装后的直接进行切片</span></span><br><span class="line">        x,y = Data[batch*batch_size:batch*batch_size+batch_size]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#预测、计算损失</span></span><br><span class="line">        y_pred = model(x)</span><br><span class="line">        y_loss = loss(y,y_pred)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#每次循环梯度清零</span></span><br><span class="line">        opt.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#优化</span></span><br><span class="line">        opt.step()</span><br></pre></td></tr></table></figure>



<h2 id="Dataloader类"><a href="#Dataloader类" class="headerlink" title="Dataloader类"></a>Dataloader类</h2><p>为了防止模型在顺序切片中学习到顺序知识，可以采用Dataloader类进行数据处理。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataloader</span><br><span class="line"></span><br><span class="line">Data = TensorDataset(X,Y)</span><br><span class="line"><span class="comment">#每次会提供 batch_size的数量，并且进行乱序处理</span></span><br><span class="line">Data_loa = DataLoader(Data,batch_size = batch_size,shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoches):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> Data_loa:</span><br><span class="line">        <span class="comment">#预测、计算损失</span></span><br><span class="line">        y_pred = model(x)</span><br><span class="line">        y_loss = loss(y_pred,y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#每次循环梯度清零</span></span><br><span class="line">        opt.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#优化</span></span><br><span class="line">        opt.step()</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型参数</span></span><br><span class="line">model.state_dict()</span><br></pre></td></tr></table></figure>



<h2 id="自己的数据集"><a href="#自己的数据集" class="headerlink" title="自己的数据集"></a>自己的数据集</h2><p>上面的做法都是torch为我们提供好了的数据集，以天气数据集为例。</p>
<h3 id="torchvision-datasets-ImageFolder"><a href="#torchvision-datasets-ImageFolder" class="headerlink" title="torchvision.datasets.ImageFolder()"></a>torchvision.datasets.ImageFolder()</h3><p>该方法可以从分类文件夹中创建dataset数据类型的数据。要求每个属于同一类别的图片存放在同一个文件夹下。</p>
<p><strong>这是原始数据集：</strong></p>
<p><img src="/../../images/image-20220628131445776.png" alt="image-20220628131445776"></p>
<p>各个类别都是放在一起的，所以首先需要先对数据集进行分类。</p>
<p><strong>划分：</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil    <span class="comment">#用于复制图片</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#划分数据集后需要存放的根目录</span></span><br><span class="line">base_dir = <span class="string">r&quot;./data/weather&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果不存在就创建</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(base_dir):</span><br><span class="line">    os.mkdir(base_dir)</span><br><span class="line">    <span class="comment">#训练数据就在根目录后加一个train</span></span><br><span class="line">    train_dir = os.path.join(base_dir,<span class="string">&quot;train&quot;</span>)</span><br><span class="line">    tesr_dir = os.path.join(base_dir,<span class="string">&quot;test&quot;</span>)</span><br><span class="line">    os.mkdir(train_dir)</span><br><span class="line">    os.mkdir(tesr_dir)</span><br><span class="line">    </span><br><span class="line">data_list = [<span class="string">&#x27;rain&#x27;</span>,<span class="string">&#x27;shine&#x27;</span>,<span class="string">&#x27;cloudy&#x27;</span>,<span class="string">&#x27;sunrise&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train_or_test <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>,<span class="string">&#x27;test&#x27;</span>]:</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> data_list:</span><br><span class="line">        os.mkdir(os.path.join(base_dir,train_or_test,data))</span><br><span class="line">        </span><br><span class="line">img_dir = <span class="string">r&quot;./data/dataset2&quot;</span></span><br><span class="line"><span class="comment">#测试列出所有文件</span></span><br><span class="line"><span class="comment">#os.listdir(img_dir)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#加序号，将能被五整除的放在test中作为测试</span></span><br><span class="line"><span class="keyword">for</span> i,name <span class="keyword">in</span> <span class="built_in">enumerate</span>(os.listdir(img_dir)):</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> data_list:</span><br><span class="line">        <span class="keyword">if</span> data <span class="keyword">in</span> name:</span><br><span class="line">            <span class="comment">#原始图片位置</span></span><br><span class="line">            source = os.path.join(img_dir,name)</span><br><span class="line">            <span class="keyword">if</span> i%<span class="number">5</span>==<span class="number">0</span>:</span><br><span class="line">                <span class="comment">#目标目录</span></span><br><span class="line">                d = os.path.join(base_dir,<span class="string">&#x27;test&#x27;</span>,data,name)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                d = os.path.join(base_dir,<span class="string">&#x27;train&#x27;</span>,data,name)</span><br><span class="line">            <span class="comment">#图片拷贝，源地址-&gt;目标地址</span></span><br><span class="line">            shutil.copy(source,d)</span><br></pre></td></tr></table></figure>



<p><strong>读取、预处理：</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="comment">#规定转换规则</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">                            transforms.Resize((<span class="number">96</span>,<span class="number">96</span>)),</span><br><span class="line">                            transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"><span class="comment">#从图片文件见加载数据</span></span><br><span class="line"></span><br><span class="line">train_ds = torchvision.datasets.ImageFolder(train_dir,transform)</span><br><span class="line">test_ds = torchvision.datasets.ImageFolder(test_dir,transform)</span><br><span class="line"></span><br><span class="line">train_ds.classes			<span class="comment">#查看类别</span></span><br><span class="line"></span><br><span class="line">train_ds.class_to_idx		<span class="comment">#类别+编号	&#123;&#x27;cloudy&#x27;: 0, &#x27;rain&#x27;: 									1, &#x27;shine&#x27;: 2, &#x27;sunrise&#x27;: 3&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#dataset 创建dataloader</span></span><br><span class="line">train_dl = DataLoader(train_ds,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dl = DataLoader(test_ds,batch_size=<span class="number">64</span>,shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>



<h3 id="创建data-Dataset的子类"><a href="#创建data-Dataset的子类" class="headerlink" title="创建data.Dataset的子类"></a>创建data.Dataset的子类</h3><p>上一种方式由于需要自己先对图片进行划分、分类，并且还需要使用到  os和shutill库，对这些库如果不熟悉的话比较难以操作。</p>
<p>所以这里介绍的这种方法使用的是创建Dataset的子类，来构建数据集输入，因为pytorch内置的数据集的格式就是Dataset类型，通过继承这个类来重写构建数据集的这种方法会更加灵活。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#使用这个库获取一个文件夹下，所有文件的路径</span></span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="comment">#获取路径下所有以jpg结尾的图片</span></span><br><span class="line">all_img_path = glob.glob(<span class="string">r&quot;C:\Users\zyh\Desktop\Python\data\dataset2\*.jpg&quot;</span>)</span><br><span class="line">data_list = [<span class="string">&#x27;rain&#x27;</span>,<span class="string">&#x27;shine&#x27;</span>,<span class="string">&#x27;cloudy&#x27;</span>,<span class="string">&#x27;sunrise&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建 类别 到 标号的字典</span></span><br><span class="line">data_to_index = <span class="built_in">dict</span>((c,i) <span class="keyword">for</span> i,c <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_list))</span><br><span class="line"><span class="comment">#构建标号 到 类别的字典   .items()方法可以将每个字典的元组进行返回</span></span><br><span class="line">index_to_data = <span class="built_in">dict</span>((v,k) <span class="keyword">for</span> k,v <span class="keyword">in</span> data_to_index.items())     </span><br><span class="line"><span class="comment">#用于存放每一个标签</span></span><br><span class="line">all_label = []</span><br><span class="line"><span class="keyword">for</span> img <span class="keyword">in</span> all_img_path:</span><br><span class="line">    <span class="keyword">for</span> i,name <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_list):</span><br><span class="line">        <span class="keyword">if</span> name <span class="keyword">in</span> img:</span><br><span class="line">            all_label.append(i)</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line"><span class="comment">#由于这个数据和标签都是有顺序，分好类的为防止模型学习到顺序信息，需要对模型进行乱序</span></span><br><span class="line"><span class="comment">#做乱序的同时又需要保证，图片和标签还是一一对应的，所以需要构建一个乱序的index</span></span><br><span class="line"><span class="comment">#permutation 会返回一个乱序的下标</span></span><br><span class="line">index = np.random.permutation(<span class="built_in">len</span>(all_img_path))       </span><br><span class="line"></span><br><span class="line"><span class="comment">#按照index进行排序</span></span><br><span class="line">all_img_path = np.array(all_img_path)[index]</span><br><span class="line">all_label = np.array(all_label)[index]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#80% 作为训练</span></span><br><span class="line">lens = <span class="built_in">int</span>(<span class="built_in">len</span>(all_img_path)*<span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line">train_img_path = all_img_path[<span class="number">0</span>:lens]</span><br><span class="line">test_img_path = all_img_path[lens:]</span><br><span class="line"></span><br><span class="line">train_label = all_label[<span class="number">0</span>:lens]</span><br><span class="line">test_label = all_label[lens:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">96</span>,<span class="number">96</span>)),</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义Dataset的子类</span></span><br><span class="line"><span class="comment">#需要继承data.Dataset 这个类</span></span><br><span class="line"><span class="comment">#必须重写__getitem__这个方法，__len__</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mydataset</span>(data.Dataset):</span><br><span class="line">    <span class="comment">#需要传入所有图片的路径</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,img_path,label_path,transform</span>):</span><br><span class="line">        self.img_path = img_path</span><br><span class="line">        self.label_path = label_path</span><br><span class="line">        self.transform = transform</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#使得这个定义的类，可以使用索引、切片</span></span><br><span class="line">    <span class="comment">#这种构建数据集的方式灵活的地方就在于</span></span><br><span class="line">    <span class="comment">#在实现__getitem__方法的同时，可以加入一些需要进行的操作</span></span><br><span class="line">    <span class="comment">#因为通过该类实例化出来的对象一定是Dataset的子类，可以直接用于构建Dataloader</span></span><br><span class="line">    <span class="comment">#而在构建dataloader的时候，一定会用到索引从而执行__getitem__方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,index</span>):</span><br><span class="line">        img = self.img_path[index]</span><br><span class="line">        label = self.label_path[index]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#打开图片，并完成tensor的转换</span></span><br><span class="line">        read_pic = Image.<span class="built_in">open</span>(img)	<span class="comment">#H,W</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#如果数据集中出现一张存在问题的图片，其他都是彩色图片，而有一张		是黑白的，如果直接传入模型就会出现问题</span></span><br><span class="line">        <span class="comment">#所以需要对打开的图片转为ndarray的形式，判断是否是一张正常的图			片，若不是则添加上一个维度并且让这个维度重复三次。</span></span><br><span class="line">        np_pic = np.asarray(read_pic,dtype=np.uint8)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(np_pic.shape)==<span class="number">2</span>:</span><br><span class="line">            <span class="comment">#添加一个数据为1的维度</span></span><br><span class="line">            img_data = np_pic[:,:,np.newaxis]</span><br><span class="line">            <span class="comment">#重复三次，在第二维进行重复</span></span><br><span class="line">            np.repeat(img_data,<span class="number">3</span>,axis=<span class="number">2</span>)</span><br><span class="line">            <span class="comment">#ndarray转回Image形式</span></span><br><span class="line">            read_pic = Image.fromarray(img_data)</span><br><span class="line">            </span><br><span class="line">        data = self.transform(read_pic)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> data,label</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)</span><br><span class="line">    </span><br><span class="line">train_ds = Mydataset(train_img_path,train_label,transform)</span><br><span class="line">test_ds = Mydataset(test_img_path,test_label,transform)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_dl = data.DataLoader(train_ds,batch_size=<span class="number">32</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dl = data.DataLoader(test_ds,batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure>





<h1 id="线性模型创建"><a href="#线性模型创建" class="headerlink" title="线性模型创建"></a>线性模型创建</h1><p><strong>通过torch中的核心nn中的Sequential() 方法来进行构建网络模型</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> toch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">#导入数据</span></span><br><span class="line">X = ...</span><br><span class="line">Y = ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#构建模型</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">			....</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建损失函数，训练时可以通过，loss(model(x),y) 来查看损失。由于损失是不需要计算梯度的，所以计算损失时，需要用 with torch.no_grad():</span></span><br><span class="line">loss = nn.xxxLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建优化函数,以Adam为例,传入模型需要优化的参数、lr</span></span><br><span class="line">opt = torch.optim.Adam(model.(),lr)</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定batch_size,以及数据集可以分为几个批次</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">num_of_batch = <span class="built_in">len</span>(num)//batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定epoch</span></span><br><span class="line">epoches = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoches):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(num_of_batch):</span><br><span class="line">        <span class="comment">#每次开始、结束的位置</span></span><br><span class="line">        start = batch*batch_size</span><br><span class="line">        end = start+batch_size</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#数据切分</span></span><br><span class="line">        x = X[start:end]</span><br><span class="line">        y = Y[start:end]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#预测、计算损失</span></span><br><span class="line">        y_pred = model(x)</span><br><span class="line">        y_loss = loss(y,y_pred)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#每次循环梯度清零</span></span><br><span class="line">        opt.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#优化</span></span><br><span class="line">        opt.step()</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型参数</span></span><br><span class="line">model.state_dict()</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<h2 id="数据划分"><a href="#数据划分" class="headerlink" title="数据划分"></a>数据划分</h2><p>借助sklearn库中的train_test_split 进行划分</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset,DataLoader</span><br><span class="line"><span class="comment">#划分</span></span><br><span class="line">train_x,test_x,train_y.test_y = train_test_split(X,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据类型转换,torch.float32 和 torch.FloatTensor是一样的</span></span><br><span class="line">train_x = torch.from_numpy(train_x).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">train_y = torch.from_numpy(train_y).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">test_x = torch.from_numpy(test_x).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">test_y = torch.from_numpy(test_y).<span class="built_in">type</span>(torch.float32)</span><br><span class="line"><span class="comment">#包装</span></span><br><span class="line">train_ds = TensorDataset(train_x,train_y)</span><br><span class="line">train_dl = DataLoader(train_ds,batch_szie = btatch_Size,shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_ds = TensorDataset(test_x,test_y)</span><br><span class="line">test_dl = DataLoader(test_ds,batch_szie = btatch_Size)</span><br></pre></td></tr></table></figure>





<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>常用的损失函数包括 ，均方误差、交叉熵、三元组..。这里对比交叉熵损失中的<strong>BCELoss</strong> 和 <strong>CrossEntropyLoss</strong> 的区别。</p>
<blockquote>
<p><font color=red>BCELoss</font>:</p>
<p>BCELoss可以看作是CrossEntropyLoss的一个二分类问题上的特例，在使用时，BCELoss的输入的真实值是需要经过 one-hot的编码的，并且BCELoss的输入预测值是需要经过sofmax()层的。</p>
<p><font color=red>CrossEntropyLoss</font>：</p>
<p>CrossEntropyLoss是多元交叉熵损失，和BCELoss不同的是在使用时，真实值不需要经过one-hot，而是直接采用类别标签索引，输入预测值不需要经过sofmax()激活，CrossEntropyLoss会自己完成sofmax，以提高效率。</p>
</blockquote>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>torch的所有优化器都在 torch.optim 模块下。最常见的有 SGD、Adam….。</p>
<p>Adam优化器是较优秀的一个优化算法，它在训练过程中，随着权重的不断迭代，学习率也会自适应的跟新，为参数提供独立自适应的学习率。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Moduel):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__();</span><br><span class="line">        ...</span><br><span class="line">    <span class="comment">#网络模型，定义每一层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        self.xxx = </span><br><span class="line">        self.xxx1 = </span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_availabel() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>        </span><br><span class="line"></span><br><span class="line">model = Model().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment">#优化器的参数是模型可训练参数和学习率</span></span><br><span class="line">optim = torch.nn.optim(model.parameters(),lr = <span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>



<h2 id="训练函数"><a href="#训练函数" class="headerlink" title="训练函数"></a>训练函数</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_dl,model,loss,optim</span>):</span><br><span class="line">    <span class="comment">#train_dl.dataset 会返回创建dataloader的dataset，获取当前总数据的大小</span></span><br><span class="line">    size = <span class="built_in">len</span>(train_dl.dataset)</span><br><span class="line">    <span class="comment">#在创建dataloader对象train_dl时候指定了batch_size，所以len(train_dl) 会得到要训练一次的总的 批次数</span></span><br><span class="line">    num_batches = <span class="built_in">len</span>(train_dl)</span><br><span class="line">    </span><br><span class="line">    train_loss,correct = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> tran_dl:</span><br><span class="line">        x,y = x.to(device),y.to(device)</span><br><span class="line">        </span><br><span class="line">        pred = model(x)</span><br><span class="line">        </span><br><span class="line">        Loss = loss(pred,y)</span><br><span class="line">        </span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        </span><br><span class="line">        loss.backword()</span><br><span class="line">        </span><br><span class="line">        optim.step()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment">#每一个批次预测正确的加起来，sum()是求本批次预测正确的个数。由于结果是tensor对象要用item()转为python对象进行累加。</span></span><br><span class="line">            correct+=(pred.argmax(axis=<span class="number">1</span>)==y).<span class="built_in">type</span>(torch.float32).<span class="built_in">sum</span>().item()</span><br><span class="line">            </span><br><span class="line">            train_loss+= Loss.item()</span><br><span class="line">            </span><br><span class="line">	acc = correct/size</span><br><span class="line">    train_loss = train_loss/num_batches</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> acc,train_loss</span><br></pre></td></tr></table></figure>



<h2 id="测试函数"><a href="#测试函数" class="headerlink" title="测试函数"></a>测试函数</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">test_dl,model,loss</span>):</span><br><span class="line">    size = <span class="built_in">len</span>(test_dl.dataset)</span><br><span class="line"></span><br><span class="line">    num_batches = <span class="built_in">len</span>(test_dl)</span><br><span class="line">    </span><br><span class="line">    test_loss,correct = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="comment">#测试不需要计算梯度</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> x,y <span class="keyword">in</span> test_dl:</span><br><span class="line">            x,y = x.to(device),y.to(device)</span><br><span class="line"></span><br><span class="line">            pred = model(x)</span><br><span class="line"></span><br><span class="line">            Loss = loss(pred,y)</span><br><span class="line"></span><br><span class="line">            test_loss+=Loss.item()</span><br><span class="line">            </span><br><span class="line">            correct+=(pred.argmax(axis=<span class="number">1</span>)==y).<span class="built_in">type</span>(torch.float32).<span class="built_in">sum</span>().item()</span><br><span class="line">		acc = correct/size</span><br><span class="line">		train_loss = train_loss/num_batches</span><br><span class="line">    </span><br><span class="line">   		 <span class="keyword">return</span> acc,train_loss</span><br></pre></td></tr></table></figure>



<h2 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">50</span></span><br><span class="line">train_loss=[]</span><br><span class="line">train_acc=[]</span><br><span class="line">test_loss=[]</span><br><span class="line">test_acc=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    epoch_train_acc,epoch_train_loss = train(train_dl,model,loss,optim)</span><br><span class="line">    </span><br><span class="line">      epoch_test_acc,epoch_test_loss = test(test_dl,model,loss)</span><br><span class="line">        </span><br><span class="line">    train_loss.append(epoch_train_loss)</span><br><span class="line">    train_acc.append(epoch_train_acc)</span><br><span class="line">    </span><br><span class="line">    test_loss.append(epoch_test_loss)</span><br><span class="line">    test_acc.append(epoch_test_acc)</span><br></pre></td></tr></table></figure>





<h1 id="卷积模型创建"><a href="#卷积模型创建" class="headerlink" title="卷积模型创建"></a>卷积模型创建</h1><p>和线性模型类似，其他代码几乎不需要做改变，只需要在构建模型时，采用卷积层、下采样层最后再加上全连接层。</p>
<p><strong>修改后：</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,<span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d((<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>)</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">256</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        x = torch.relu(self.conv1(<span class="built_in">input</span>))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = torch.relu(self.conv2(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="built_in">print</span>(x.size())         <span class="comment">#打印大小 补全self.linear1 的输入</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#展平</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        x = torch.relu(self.linear1(x))</span><br><span class="line">        out = self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印测试 print(x)</span></span><br><span class="line">img,label = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_dl))</span><br><span class="line">model(img)</span><br></pre></td></tr></table></figure>



<h2 id="过拟合处理–dropout层"><a href="#过拟合处理–dropout层" class="headerlink" title="过拟合处理–dropout层"></a>过拟合处理–dropout层</h2><p>过拟合本质就是模型泛化能力不行，模型只能够学习到训练给的数据，结果过拟合最根本的方法当然是增加训练数据。但是往往我们数据集是有限的，要在有限的数据集上防止过拟合就要用到 <strong>dropout层</strong> ，dropout的做法是在一次前向传播过程中以一定的概率让某些神经元失效不进行学习。并且他只在训练时生效，可以理解为在训练时，添加上dropout层之后 ，模型在训练时，不需要使用所有的神经元，而让任意神经元直接相互组合都已经能够解决问题，在做预测时使用所有神经元更能够解决问题。</p>
<p><strong>添加dropout层</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,<span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d((<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>)</span><br><span class="line">        <span class="comment">#添加dropout层，线性层</span></span><br><span class="line">        self.drop = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        <span class="comment">#卷积层</span></span><br><span class="line">        self.drop2 = nn.Dropout2d(<span class="number">0.5</span>)</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">256</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        x = torch.relu(self.conv1(<span class="built_in">input</span>))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = torch.relu(self.conv2(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="built_in">print</span>(x.size())         <span class="comment">#打印大小 补全self.linear1 的输入</span></span><br><span class="line">        <span class="comment">#卷积层使用dropout</span></span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#展平</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        x = torch.relu(self.linear1(x))</span><br><span class="line">        <span class="comment">#线性层使用droput层</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        out = self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印测试 print(x)</span></span><br><span class="line"><span class="comment">#img,label = next(iter(test_dl))</span></span><br><span class="line"><span class="comment">#model(img)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#注：在使用droput进行训练时，要注意训练函数中需要指定当前模型是进行训练还是测试，通过在训练代码之前加上 model.train()和在预测代码前面加上 model.eval() 进行指定。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h1 id="批标准化"><a href="#批标准化" class="headerlink" title="批标准化"></a>批标准化</h1><p><strong>标准化：</strong> 是指将数据映射到某一指定的范围，除去量纲带来的影响。做法<strong>（数据-均值）&#x2F;方差</strong></p>
<p><strong>归一化：</strong> 将数据取值规定在0-1之间。他们都是数据标准化常见的做法。</p>
<p><strong>批标准化：</strong> 和标准化类似，只是批标准化不仅仅考虑输入时数据进行标准化，在数据在模型中进行运算的过程中，每个中间层的数据同样会进行标准化之后才会作为下一层的输入，他和dropout层一样只有在训练时有效。 BN层具有一下优点：</p>
<p>​		（1）具有正则化效果</p>
<p>​		（2）提高泛化能力</p>
<p>​		（3）提高学习速率</p>
<p>过程：</p>
<p>​		（1）求每一个训练批次的均值和方差</p>
<p>​		（2）数据标准化</p>
<p>​		（3）训练超参数γ 和β</p>
<p><strong>添加BN层</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,<span class="number">5</span>)</span><br><span class="line">        <span class="comment">#卷积层使用BatchNorm2d，参数为上一层卷积输出的通道数</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">6</span>)</span><br><span class="line">        </span><br><span class="line">        self.pool = nn.MaxPool2d((<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>)</span><br><span class="line">         <span class="comment">#卷积层使用BatchNorm2d，参数为上一层卷积输出的通道数</span></span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">16</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#添加dropout层，线性层</span></span><br><span class="line">        self.drop = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        <span class="comment">#卷积层</span></span><br><span class="line">        self.drop2 = nn.Dropout2d(<span class="number">0.5</span>)</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">256</span>)</span><br><span class="line">        <span class="comment">#线性层使用BatchNorm1d,参数为上一个全连接层的输出单元数</span></span><br><span class="line">        self.bn_f1 = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        </span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        x = torch.relu(self.conv1(<span class="built_in">input</span>))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="comment">#卷积层使用BN</span></span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        </span><br><span class="line">        x = torch.relu(self.conv2(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="comment">#卷积层使用BN</span></span><br><span class="line">        x= self.bn2(x)</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(x.size())         <span class="comment">#打印大小 补全self.linear1 的输入</span></span><br><span class="line">        <span class="comment">#卷积层使用dropout</span></span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#展平</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        x = torch.relu(self.linear1(x))</span><br><span class="line">        <span class="comment">#线性层使用BN</span></span><br><span class="line">        x = self.bn_f1 (x)</span><br><span class="line">        <span class="comment">#线性层使用droput层</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        out = self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h1 id="预训练网络"><a href="#预训练网络" class="headerlink" title="预训练网络"></a>预训练网络</h1><p>预训练网络是指一个保存好的之前已在大规模数据集上训练好的卷积神经网络。这个网络在原始任务中已经达到较好的效果，被证明是可以完成视觉任务的，可以完成特征提取的网络。所以在全新的任务中使用其网络模型，在不同问题之间的特征是可移植的。也就是说预训练网络的卷积部分已经是肯定能够很好的提取特征的了，只需要在全连接部分进行修改，训练一个新的分类器就可以了。</p>
<p>pytorch内置的预训练网络包括，VGG16，VGG19，densenet，ResNet..</p>
<p>这些模型都是在  ImageNet上进行预训练的，这是一个专门为机器视觉研究而手动标注好类别的图片数据库，有22000个类别。</p>
<h2 id="使用pytorch的内置模型"><a href="#使用pytorch的内置模型" class="headerlink" title="使用pytorch的内置模型"></a>使用pytorch的内置模型</h2><p>torch的所有内置模型都在，torchvision.model这个模块下，下面包括很多模型。</p>
<p><img src="/../../images/image-20220701100428236.png" alt="image-20220701100428236"></p>
<h2 id="VGG16"><a href="#VGG16" class="headerlink" title="VGG16"></a>VGG16</h2><p>这是VGG16 的参数，第一个参数表示是否使用在Imagenet上训练好的预训练权重。</p>
<p><img src="/../../images/image-20220701100635491.png" alt="image-20220701100635491"></p>
<p>这个是权重保存的 位置，下次在使用时就会直接在这里进行读取。</p>
<p><img src="/../../images/image-20220701100909004.png" alt="image-20220701100909004"></p>
<p>使用方法：由于使用了预训练权重，所以卷积部分是一定可以有效的提取特征的 了，所以不需要再进行训练，将其冻结，在分类器部分需要重新初始化，让模型适应现在的输入。以下就是VGG的结构：</p>
<p><img src="/../../images/image-20220701101700486.png" alt="image-20220701101700486"></p>
<p>可以根据model.features、model.classifier选择提取出卷积层和全连接层。并且提取出的每一个层都可以通过索引进行操作。<strong>如要操作classifier的Linear层的out_features</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.classifier[<span class="number">6</span>].out_features		<span class="comment">#1000</span></span><br></pre></td></tr></table></figure>





<p>由于卷积部分不需要进行训练所以要将卷积部分的参数设置为不需要计算梯度，</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> model.features.parameters():</span><br><span class="line">    p.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> model.classifier.parameters():</span><br><span class="line">    c.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure>



<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将VGG16使用在天气数据集上</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil    <span class="comment">#用于复制图片</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment">#将训练和测试放在同一个方法里，fit方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">epoch,model,train_dl,test_dl</span>):</span><br><span class="line">    size = <span class="built_in">len</span>(train_dl.dataset)</span><br><span class="line">    correct,total,running_loss = <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="comment">#设置当前为训练模式</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> train_dl:</span><br><span class="line">        x,y = x.to(device),y.to(device)</span><br><span class="line">        pred = model(x)</span><br><span class="line">        loss = loss_fn(pred,y)</span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optim.step()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            correct+=(pred.argmax(axis=<span class="number">1</span>) == y).<span class="built_in">type</span>(torch.float32).<span class="built_in">sum</span>().item()</span><br><span class="line">            total+=y.size(<span class="number">0</span>)</span><br><span class="line">            running_loss+=loss.item()</span><br><span class="line">    train_epoch_loss = running_loss/size</span><br><span class="line">    train_epoch_acc = correct/total</span><br><span class="line">    </span><br><span class="line">    test_correct,test_loss,test_running_loss,test_total= <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    test_size = <span class="built_in">len</span>(test_dl.dataset)</span><br><span class="line">    <span class="comment">#设置当前为测试模式</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> x,y <span class="keyword">in</span> test_dl:</span><br><span class="line">            x,y = x.to(device),y.to(device)</span><br><span class="line">            pred = model(x)</span><br><span class="line">            loss = loss_fn(pred,y)</span><br><span class="line">            test_correct+=(pred.argmax(axis=<span class="number">1</span>)==y).<span class="built_in">type</span>(torch.float32).<span class="built_in">sum</span>().item()</span><br><span class="line">            test_running_loss+=loss.item()</span><br><span class="line">            test_total+=y.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        test_epoch_loss = test_running_loss/test_size</span><br><span class="line">        test_epoch_acc = test_correct/test_total</span><br><span class="line">        </span><br><span class="line">    template = <span class="string">&quot;epoch:&#123;:2d&#125;,train_loss:&#123;:.5f&#125;,train_acc:&#123;:.5f&#125;,test_loss:&#123;:.5f&#125;,test_acc:&#123;:.5f&#125;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(template.<span class="built_in">format</span>(epoch,train_epoch_loss,train_epoch_acc,test_epoch_loss,test_epoch_acc))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train_epoch_loss,train_epoch_acc,test_epoch_loss,test_epoch_acc</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    base_dir = <span class="string">r&quot;./data/weather&quot;</span></span><br><span class="line">    train_dir = os.path.join(base_dir,<span class="string">&quot;train&quot;</span>)</span><br><span class="line">    test_dir = os.path.join(base_dir,<span class="string">&quot;test&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">                transforms.Resize((<span class="number">192</span>,<span class="number">192</span>)),</span><br><span class="line">                    transforms.ToTensor()</span><br><span class="line">    ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_ds = torchvision.datasets.ImageFolder(train_dir,transform)</span><br><span class="line">    test_ds = torchvision.datasets.ImageFolder(test_dir,transform)</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">64</span></span><br><span class="line">    train_dl = DataLoader(train_ds,batch_size,shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_dl = DataLoader(test_ds,batch_size)</span><br><span class="line"></span><br><span class="line">    model = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.features.parameters():</span><br><span class="line">        p.requires_grad = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> model.classifier.parameters():</span><br><span class="line">        c.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#迁移到四分类上</span></span><br><span class="line">    model.classifier[<span class="number">6</span>].out_features = <span class="number">4</span></span><br><span class="line">    <span class="comment">#只需要优化分类器</span></span><br><span class="line">    optim = torch.optim.Adam(model.classifier.parameters()，<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">    </span><br><span class="line">    epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    train_epoch_loss,train_epoch_acc,test_epoch_loss,test_epoch_acc = fit(epoch,model,train_dl,test_dl)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="/../../images/image-20220701111142494.png" alt="image-20220701111142494"></p>
<p>可以看到使用预训练模型 模型很快就能够达到很高的正确率，但是在后面几个epoch可以看出，train_loss 还在上升，但是test_loss已经开始下降了，这就是很明显的过拟合现象。</p>
<p>在使用预训练模型的时候由于模型的结构已经固定了，所以不能够通过添加<strong>Dropout、BN</strong>来解决过拟合，所以只能添加训练数据，但是现在训练数据又是有限的情况，要增加训练图片的数量就只能够通过数据增强。</p>
<h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p><strong>数据增强：</strong> 对现有的数据集进行一定的变化，如反转、降低亮度…，以达到增加训练集的目的，所以只需要在划分的训练数据上进行增强。这就要使用到torchvision中非常中的一个模块<strong>transforms</strong> 这个模块为我们提供了很多图像变化的操作。</p>
<p><img src="/../../images/image-20220701111935436.png" alt="image-20220701111935436"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line">transforms.RandomCrop					<span class="comment">#随机位置裁剪</span></span><br><span class="line">transforms.RandomHorizontalFlip(p) 		<span class="comment">#以p概率水平翻转</span></span><br><span class="line">transforms.RandomVerticalFlip(p)		<span class="comment">#以p概率水平翻转</span></span><br><span class="line">transforms.RandomRotation(p)			<span class="comment">#以p概率旋转角度</span></span><br><span class="line">transforms.RandomGrayscale(p)			<span class="comment">#以p概率进行灰度化</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#颜色修改</span></span><br><span class="line">transforms.ColorJitter(brightness=)		<span class="comment">#随机调整明亮度</span></span><br><span class="line">transforms.ColorJitter(contrast=)		<span class="comment">#随即调整对比度</span></span><br><span class="line">transforms.ColorJitter(saturation=)		<span class="comment">#随即调整饱和度</span></span><br><span class="line">transforms.ColorJitter(hue=)			<span class="comment">#随即调整颜色</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>使用:</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">base_dir = <span class="string">r&quot;./data/weather&quot;</span></span><br><span class="line">   train_dir = os.path.join(base_dir,<span class="string">&quot;train&quot;</span>)</span><br><span class="line">   test_dir = os.path.join(base_dir,<span class="string">&quot;test&quot;</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line">   </span><br><span class="line">   train_transform = transforms.Compose([</span><br><span class="line">               transforms.Resize((<span class="number">192</span>,<span class="number">192</span>)),</span><br><span class="line">       		<span class="comment">#随机位置裁剪大小为64</span></span><br><span class="line">       		transforms.RandomCrop(<span class="number">64</span>),</span><br><span class="line">       		<span class="comment">#以0.5概率水平翻转</span></span><br><span class="line">       		transforms.RandomHorizontalFlip(<span class="number">0.5</span>),</span><br><span class="line">       		<span class="comment">#旋转20度</span></span><br><span class="line">       		transforms.RandomRotation(<span class="number">0.2</span>),</span><br><span class="line">       		<span class="comment">#随机明暗度</span></span><br><span class="line">       		transforms.ColorJitter(brightness=<span class="number">0.5</span>)</span><br><span class="line">       		transforms.ToTensor()</span><br><span class="line">   ]</span><br><span class="line">   )</span><br><span class="line">   test_transform = transforms.Compose([</span><br><span class="line">               transforms.Resize((<span class="number">192</span>,<span class="number">192</span>)),</span><br><span class="line">                   transforms.ToTensor()</span><br><span class="line">   ]</span><br><span class="line">   )</span><br><span class="line">   train_ds = torchvision.datasets.ImageFolder(train_dir,train_transform)</span><br><span class="line">   test_ds = torchvision.datasets.ImageFolder(test_dir,test_transform)</span><br><span class="line"></span><br><span class="line">   batch_size = <span class="number">64</span></span><br><span class="line">   train_dl = DataLoader(train_ds,batch_size,shuffle=<span class="literal">True</span>)</span><br><span class="line">   test_dl = DataLoader(test_ds,batch_size)</span><br></pre></td></tr></table></figure>



<h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><p>在训练过程中，可能在接近极值点的时候，由于学习率初始设定可能较大，导致一直在极值点左右来回震荡，学习率设置的较小，导致训练时间漫长始终无法到达极值点。所以希望在一开始的时候，学习率大慢慢的随着训练的迭代，学习率可以进行衰减。</p>
<p>在优化器<strong>optim</strong>中包括了训练所有必要的参数，同样学习率也不例外。</p>
<p><img src="/../../images/image-20220701141148868.png" alt="image-20220701141148868"></p>
<p>可以通过 <strong>optim.param_groups[‘lr’]</strong> 获得学习率，并且进行修改。但是这样就需要在训练中进行修改，比较麻烦。pytorch为我们提供了一个分装好了的学习率衰减模块，<strong>lr_scheduler</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="comment">#每隔五步进行一次衰减，衰减方式为  lr*=0.9</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optim,step_size=<span class="number">5</span>,gamma=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在第20,50,80个epoch进行衰减</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.MultiStepLR(optim,[<span class="number">20</span>,<span class="number">50</span>,<span class="number">80</span>],gamma=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个epoch进行衰减</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.ExponentialLR(optim,gamma=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用：只需要在每个epoch结束时，调用exp_lr_scheduler.setp()方法</span></span><br></pre></td></tr></table></figure>



<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>使用方法、模型导入和VGG16 一样。</p>
<p>这是ResNet18的网络模型结构：</p>
<p><img src="/../../images/image-20220701143254501.png" alt="image-20220701143254501"></p>
<p>使用内置模型大部分操作都一样，只需要修改全连接层，这里记录另一种修改模型的方式</p>
<p><img src="/../../images/image-20220701144639117.png" alt="image-20220701144639117"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在使用内置模型时，一般卷积部分是不需要修改的，只需要修改全连接层分类的个数，所以可以直接修改模型全连接层的输出的通道数</span></span><br><span class="line"></span><br><span class="line">model.fc.out_features = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#实际上也可以直接对整个Linear层进行修改，重新创建一个model.fc的Linear层,自己定义新的Linear层的话，默认的参数就是可训练的。</span></span><br><span class="line">in_f= model.fc.in_features</span><br><span class="line">model.fc = nn.Linear(in_f,<span class="number">4</span>)</span><br></pre></td></tr></table></figure>



<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><p>上面使用预训练模型，都是只修改分类器，但是卷积部分虽然是能够提取有效的特征的，但是卷积模型也是对Imagenet的输入较为敏感，为了进一步提升，可以将卷积的也加入使用，但是要求是先冻结卷积模型的前提下修改分类器准确率较好的情况下再解冻卷积进行训练。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#...</span></span><br><span class="line"><span class="comment">#现在分类器已经训练好，再对模型进行微调</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将模型所有参数解冻</span></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">    p.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这次修改的参数是所有的参数，微调部分学习率要比较低    </span></span><br><span class="line">optim = torch.optim.Adam(model.parameters(),lr=<span class="number">0.00001</span>)</span><br></pre></td></tr></table></figure>



<h2 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,<span class="number">5</span>)</span><br><span class="line">        <span class="comment">#卷积层使用BatchNorm2d，参数为上一层卷积输出的通道数</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">6</span>)</span><br><span class="line">        </span><br><span class="line">        self.pool = nn.MaxPool2d((<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>)</span><br><span class="line">         <span class="comment">#卷积层使用BatchNorm2d，参数为上一层卷积输出的通道数</span></span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">16</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#添加dropout层，线性层</span></span><br><span class="line">        self.drop = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        <span class="comment">#卷积层</span></span><br><span class="line">        self.drop2 = nn.Dropout2d(<span class="number">0.5</span>)</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">256</span>)</span><br><span class="line">        <span class="comment">#线性层使用BatchNorm1d,参数为上一个全连接层的输出单元数</span></span><br><span class="line">        self.bn_f1 = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        </span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        x = torch.relu(self.conv1(<span class="built_in">input</span>))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="comment">#卷积层使用BN</span></span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        </span><br><span class="line">        x = torch.relu(self.conv2(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="comment">#卷积层使用BN</span></span><br><span class="line">        x= self.bn2(x)</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(x.size())         <span class="comment">#打印大小 补全self.linear1 的输入</span></span><br><span class="line">        <span class="comment">#卷积层使用dropout</span></span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#展平</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        x = torch.relu(self.linear1(x))</span><br><span class="line">        <span class="comment">#线性层使用BN</span></span><br><span class="line">        x = self.bn_f1 (x)</span><br><span class="line">        <span class="comment">#线性层使用droput层</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        out = self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>假设这是我们的模型，经过几次训练后，需要对其进行保存。首先要想到的就是保存模型的参数，获得模型当前的参数，通过<strong>model.state_dict()</strong> 获得当前的参数状态，返回的是一个字典类型。所以只需要对这个字典进行保存就可以了。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存</span></span><br><span class="line">path = <span class="string">&quot;./model.pth&quot;</span></span><br><span class="line">torch.save(model.state_dict(),path)</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载</span></span><br><span class="line">new_model = Model()</span><br><span class="line">new_model.load_state_dict(torch.load(path))</span><br><span class="line"></span><br><span class="line"><span class="comment">#假如要保存最好的一次模型</span></span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">best_wgt = copy.deepcopy(model.state_dict())	<span class="comment">#深拷贝防止覆盖</span></span><br><span class="line">best_acc = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> eopch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    ....</span><br><span class="line">    <span class="keyword">if</span> eopch_test_acc &gt; best_acc:</span><br><span class="line">        best_wgt = copy.deepcopy(model.state_dict())</span><br><span class="line">        best_acc = epoch_test_acc</span><br><span class="line">    ....</span><br><span class="line">    </span><br><span class="line">path = <span class="string">&quot;./model.pth&quot;</span></span><br><span class="line">torch.save(best_wgt,path)</span><br></pre></td></tr></table></figure>





<h1 id="重要网络模型结构"><a href="#重要网络模型结构" class="headerlink" title="重要网络模型结构"></a>重要网络模型结构</h1><h2 id="Resnet"><a href="#Resnet" class="headerlink" title="Resnet"></a>Resnet</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>Resnet网络是国人何凯明提出的一种网络模型，该模型相比于原先的网络模型结构(学习对于输入x的输出的结果f(x)和目标的拟合能力)，但是由于深度加深可能会出现过拟合无法学习的问题。</p>
<p>Resnet模型在原先模型的基础上，将输入和输出再进行堆叠，目的就是在深层网络中，每个中间层的输出会加上来自上层的输出，所以保证模型在每次学习时，至少不会比上一次的差，一定程度上解决过拟合。下面是Resnet块的结构。</p>
<p><img src="/../../images/image-20220707193159909.png" alt="image-20220707193159909"></p>
<h3 id="Resnet残差模块实现"><a href="#Resnet残差模块实现" class="headerlink" title="Resnet残差模块实现"></a>Resnet残差模块实现</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Resnetblock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channels,out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels,out_channels,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.BN1 = nn.BatchNorm2d(out_channels)</span><br><span class="line">        </span><br><span class="line">        self.conv2 = nn.Conv2d(out_channels,out_channels,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.BN2 = nn.BatchNorm2d(out_channels)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.BN1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.BN2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        out = x+inputs</span><br><span class="line">        <span class="keyword">return</span> F.relu(out)</span><br></pre></td></tr></table></figure>





<h2 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><p>Resnet、VGG等网络大多都是只用了一种尺寸的卷积大小，Inception网络就将不同尺寸卷积组合在一起，并联合所有输出。这种做法就充分利用了不同大小卷积核的提取特征的能力，每一层都能够学习到“稀疏”（3X3，5X5）和“不稀疏”（1X1）增加了模型对不同尺寸的输入的适应性。最后经过concat进行每个特征的合成。缺点就是：计算量较大。下面是模型的结构。</p>
<p><img src="/../../images/image-20220707195433270.png" alt="image-20220707195433270"></p>
<p>为了减少训练参数可以在3X3和5X5卷积之前加上一个1X1卷积调整通道数，减少参数。</p>
<p><img src="/../../images/image-20220707202232219.png" alt="image-20220707202232219"></p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在较大型网络中可以先定义基本的一些网络结构方便复用</span></span><br><span class="line"><span class="comment">#基本结构包括：卷积+BN+激活</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Basic</span>(nn.Module):</span><br><span class="line">    <span class="comment">#还有一些其他参数用**kwargs来接收</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channels,out_channels,**kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels,out_channels,**kwargs)</span><br><span class="line">        self.BN1 = nn.BatchNorm2d(out_channels)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.BN1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义完基本模块后，就可以在目标模块上使用</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="comment">#每一层输出通道数直接指定</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1_1 = Basic(in_channels,<span class="number">64</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv2_1 = Basic(in_channels,<span class="number">64</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.conv2_3 = Basic(<span class="number">64</span>,<span class="number">96</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv3_1 = Basic(in_channels,<span class="number">48</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.conv3_5 = Basic(<span class="number">48</span>,<span class="number">64</span>,kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#只有卷积部分需要初始化，maxppool可以直接使用</span></span><br><span class="line">        self.b_pool = Basic(in_channels,<span class="number">64</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">        x1 = self.conv1_1(inputs)</span><br><span class="line">        </span><br><span class="line">        x2 = self.conv2_1(inputs)</span><br><span class="line">        x2 = self.conv2_3(x2)</span><br><span class="line">        </span><br><span class="line">        x3 = self.conv3_1(inputs)</span><br><span class="line">        x3 = self.conv3_5(x3)</span><br><span class="line">        </span><br><span class="line">        x4 = F.max_pool2d(inputs,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        x4 = self.b_pool(x4)</span><br><span class="line">        </span><br><span class="line">        out = [x1,x2,x3,x4]</span><br><span class="line">        <span class="comment">#第一个参数是tenor,第二个参数是沿着哪个维度进行合并，0：batch，1：channel，2：high，3：width</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat(out,dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>





<h2 id="Densenet"><a href="#Densenet" class="headerlink" title="Densenet"></a>Densenet</h2><h3 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h3><p>Densenet是在Resnet的基础上进行的优化，做法是每一层的输入不仅仅包含前一层的输出，而是包括前面每一层的输出都会作为这一层的输入。公式为：<strong>X<sub>l</sub> &#x3D; H(x<sub>0</sub>,x<sub>1</sub>….x<sub>l-1</sub>,)<strong>。每一层的输入不是前面层的</strong>add</strong> 而是<strong>concat</strong> 。所以在计算过程中不能改变特征图的大小，所以不能够进行下采样操作。但是有需要考虑到计算量，如果一直都是采用大小不变的特征图，计算量显然相当的大，并且在小目标的检测上可能会丢失目标。因此在设计Densent模型时，将整个模型分为若干个<strong>Densenet block</strong> ，在每个<strong>block</strong>里，特征之间是紧密连接且大小不变的，经过一个<strong>block</strong>之后，会对其进行下采样，再输入到一个新的<strong>block</strong>中。</p>
<p><img src="/../../images/image-20220708101031621.png" alt="image-20220708101031621"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://yimeng436.github.io">异梦</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://yimeng436.github.io/2022/06/15/PyTorch/PyTorch/">https://yimeng436.github.io/2022/06/15/PyTorch/PyTorch/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yimeng436.github.io" target="_blank">异梦的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="/img/top.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/06/27/FashionMNist/FashionMnist/"><img class="prev-cover" src="/img/top.png" onerror="onerror=null;src='/img/cover.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">FashionMNist</div></div></a></div><div class="next-post pull-right"><a href="/2022/06/15/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"><img class="next-cover" src="/img/top.png" onerror="onerror=null;src='/img/cover.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">多线程</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/06/06/MNIST/MNIST/" title="MNIST"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">MNIST</div></div></a></div><div><a href="/2022/06/06/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD/" title="前向传播"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">前向传播</div></div></a></div><div><a href="/2022/06/06/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="卷积神经网络"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">卷积神经网络</div></div></a></div><div><a href="/2022/06/06/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/" title="梯度下降算法"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">梯度下降算法</div></div></a></div><div><a href="/2022/06/06/tensorflow/tensorflow/" title="tensorflow"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">tensorflow</div></div></a></div><div><a href="/2022/06/27/FashionMNist/FashionMnist/" title="FashionMNist"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-27</div><div class="title">FashionMNist</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="http://mms2.baidu.com/it/u=665033858,976373917&amp;fm=253&amp;app=138&amp;f=JPEG&amp;fmt=auto&amp;q=75?w=500&amp;h=500" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">异梦</div><div class="author-info__description">欢迎访问</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">31</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/yimeng436" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2441844062@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F"><span class="toc-number">1.</span> <span class="toc-text">创建张量</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E5%88%9B%E5%BB%BA"><span class="toc-number">1.1.</span> <span class="toc-text">函数创建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.1.1.</span> <span class="toc-text">张量类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.1.2.</span> <span class="toc-text">张量类型转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%BB%B4%E5%BA%A6"><span class="toc-number">1.1.3.</span> <span class="toc-text">张量维度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor%E8%AE%BE%E5%A4%87"><span class="toc-number">1.1.4.</span> <span class="toc-text">tensor设备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97"><span class="toc-number">1.1.5.</span> <span class="toc-text">张量运算</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%8F%98%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">张量变化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#flatten-%E6%8B%89%E5%B9%B3"><span class="toc-number">2.1.</span> <span class="toc-text">flatten() 拉平</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reshape-hang-lie"><span class="toc-number">2.2.</span> <span class="toc-text">reshape(hang , lie)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#squeeze-tensor"><span class="toc-number">2.3.</span> <span class="toc-text">squeeze(tensor)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%B4%A2%E5%BC%95"><span class="toc-number">3.</span> <span class="toc-text">张量索引</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E7%BB%B4%E5%BC%A0%E9%87%8F%E7%B4%A2%E5%BC%95"><span class="toc-number">3.1.</span> <span class="toc-text">一维张量索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E7%BB%B4%E5%BC%A0%E9%87%8F%E7%B4%A2%E5%BC%95"><span class="toc-number">3.2.</span> <span class="toc-text">二维张量索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tensor-view"><span class="toc-number">3.3.</span> <span class="toc-text">tensor.view()</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%88%87%E5%88%86%E4%B8%8E%E5%90%88%E5%B9%B6"><span class="toc-number">4.</span> <span class="toc-text">张量切分与合并</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%87%E5%88%86"><span class="toc-number">4.1.</span> <span class="toc-text">切分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-chunk-tensor%EF%BC%8Cnum%EF%BC%8Cdim"><span class="toc-number">4.1.1.</span> <span class="toc-text">torch.chunk(tensor，num，dim)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-split-tensor-dim"><span class="toc-number">4.1.2.</span> <span class="toc-text">torch.split(tensor,[],dim)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%88%E5%B9%B6"><span class="toc-number">4.2.</span> <span class="toc-text">合并</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8B%BC%E6%8E%A5%EF%BC%9Acat-tensor1%EF%BC%8Ctensor2-dim-x3D-0"><span class="toc-number">4.2.1.</span> <span class="toc-text">拼接：cat([tensor1，tensor2],dim&#x3D;0)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A0%86%E5%8F%A0%EF%BC%9Astack-tensor1-tensor2-%EF%BC%8Cdim"><span class="toc-number">4.2.2.</span> <span class="toc-text">堆叠：stack([tensor1,tensor2]，dim)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">5.</span> <span class="toc-text">张量的自动微分</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#torchvision-%E5%BA%93"><span class="toc-number">6.</span> <span class="toc-text">torchvision 库</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">7.</span> <span class="toc-text">读取数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorDataset%E7%B1%BB"><span class="toc-number">7.1.</span> <span class="toc-text">TensorDataset类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataloader%E7%B1%BB"><span class="toc-number">7.2.</span> <span class="toc-text">Dataloader类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">7.3.</span> <span class="toc-text">自己的数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#torchvision-datasets-ImageFolder"><span class="toc-number">7.3.1.</span> <span class="toc-text">torchvision.datasets.ImageFolder()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAdata-Dataset%E7%9A%84%E5%AD%90%E7%B1%BB"><span class="toc-number">7.3.2.</span> <span class="toc-text">创建data.Dataset的子类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%88%9B%E5%BB%BA"><span class="toc-number">8.</span> <span class="toc-text">线性模型创建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%88%92%E5%88%86"><span class="toc-number">8.1.</span> <span class="toc-text">数据划分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">8.2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">8.3.</span> <span class="toc-text">优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0"><span class="toc-number">8.4.</span> <span class="toc-text">训练函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E5%87%BD%E6%95%B0"><span class="toc-number">8.5.</span> <span class="toc-text">测试函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="toc-number">8.6.</span> <span class="toc-text">训练循环</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E6%A8%A1%E5%9E%8B%E5%88%9B%E5%BB%BA"><span class="toc-number">9.</span> <span class="toc-text">卷积模型创建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%A4%84%E7%90%86%E2%80%93dropout%E5%B1%82"><span class="toc-number">9.1.</span> <span class="toc-text">过拟合处理–dropout层</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">10.</span> <span class="toc-text">批标准化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C"><span class="toc-number">11.</span> <span class="toc-text">预训练网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8pytorch%E7%9A%84%E5%86%85%E7%BD%AE%E6%A8%A1%E5%9E%8B"><span class="toc-number">11.1.</span> <span class="toc-text">使用pytorch的内置模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VGG16"><span class="toc-number">11.2.</span> <span class="toc-text">VGG16</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">11.3.</span> <span class="toc-text">数据增强</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="toc-number">11.4.</span> <span class="toc-text">学习率衰减</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet"><span class="toc-number">11.5.</span> <span class="toc-text">ResNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83"><span class="toc-number">11.6.</span> <span class="toc-text">微调</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98"><span class="toc-number">11.7.</span> <span class="toc-text">模型保存</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-number">12.</span> <span class="toc-text">重要网络模型结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Resnet"><span class="toc-number">12.1.</span> <span class="toc-text">Resnet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">12.1.1.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Resnet%E6%AE%8B%E5%B7%AE%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0"><span class="toc-number">12.1.2.</span> <span class="toc-text">Resnet残差模块实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Inception"><span class="toc-number">12.2.</span> <span class="toc-text">Inception</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D-1"><span class="toc-number">12.2.1.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">12.2.2.</span> <span class="toc-text">实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Densenet"><span class="toc-number">12.3.</span> <span class="toc-text">Densenet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D-2"><span class="toc-number">12.3.1.</span> <span class="toc-text">介绍</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/10/14/Paper-FSAF/Paper-FSAF/" title="FSAF论文"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="FSAF论文"/></a><div class="content"><a class="title" href="/2022/10/14/Paper-FSAF/Paper-FSAF/" title="FSAF论文">FSAF论文</a><time datetime="2022-10-13T16:00:00.000Z" title="发表于 2022-10-14 00:00:00">2022-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/25/Paper-Faster-RCNN/Paper-Faster-RCNN/" title="Faster-RCNN论文"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="Faster-RCNN论文"/></a><div class="content"><a class="title" href="/2022/08/25/Paper-Faster-RCNN/Paper-Faster-RCNN/" title="Faster-RCNN论文">Faster-RCNN论文</a><time datetime="2022-08-24T16:00:00.000Z" title="发表于 2022-08-25 00:00:00">2022-08-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/22/Redis/Redis/" title="Redis"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="Redis"/></a><div class="content"><a class="title" href="/2022/08/22/Redis/Redis/" title="Redis">Redis</a><time datetime="2022-08-21T16:00:00.000Z" title="发表于 2022-08-22 00:00:00">2022-08-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/20/MybatisPlus/MybatisPlus/" title="MybatisPlus"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="MybatisPlus"/></a><div class="content"><a class="title" href="/2022/08/20/MybatisPlus/MybatisPlus/" title="MybatisPlus">MybatisPlus</a><time datetime="2022-08-19T16:00:00.000Z" title="发表于 2022-08-20 00:00:00">2022-08-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/19/SpringBoot%E6%BA%90%E7%A0%81/SpringBoot%E6%BA%90%E7%A0%81/" title="SpringBoot源码"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="SpringBoot源码"/></a><div class="content"><a class="title" href="/2022/08/19/SpringBoot%E6%BA%90%E7%A0%81/SpringBoot%E6%BA%90%E7%A0%81/" title="SpringBoot源码">SpringBoot源码</a><time datetime="2022-08-18T16:00:00.000Z" title="发表于 2022-08-19 00:00:00">2022-08-19</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 异梦</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>