<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Paper-CV_Transformer系列 | 异梦的博客</title><meta name="keywords" content="深度学习"><meta name="author" content="异梦"><meta name="copyright" content="异梦"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="VIT">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper-CV_Transformer系列">
<meta property="og:url" content="https://yimeng436.github.io/2022/08/21/Paper-CV_Transformer%E7%B3%BB%E5%88%97/Paper-CV_Transformer%E7%B3%BB%E5%88%97/index.html">
<meta property="og:site_name" content="异梦的博客">
<meta property="og:description" content="VIT">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yimeng436.github.io/img/top.png">
<meta property="article:published_time" content="2022-08-20T16:00:00.000Z">
<meta property="article:modified_time" content="2023-09-20T04:10:38.490Z">
<meta property="article:author" content="异梦">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yimeng436.github.io/img/top.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://yimeng436.github.io/2022/08/21/Paper-CV_Transformer%E7%B3%BB%E5%88%97/Paper-CV_Transformer%E7%B3%BB%E5%88%97/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Paper-CV_Transformer系列',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-09-20 12:10:38'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="http://mms2.baidu.com/it/u=665033858,976373917&amp;fm=253&amp;app=138&amp;f=JPEG&amp;fmt=auto&amp;q=75?w=500&amp;h=500" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">62</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/top.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">异梦的博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Paper-CV_Transformer系列</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-08-20T16:00:00.000Z" title="发表于 2022-08-21 00:00:00">2022-08-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-20T04:10:38.490Z" title="更新于 2023-09-20 12:10:38">2023-09-20</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Paper-CV_Transformer系列"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><strong><center><font size=5>CV_Transformer系列</font></center></strong></p>
<h1 id="Transfomers"><a href="#Transfomers" class="headerlink" title="Transfomers"></a>Transfomers</h1><p>Transfomers 本身是<strong>NLP</strong> 中的一个方法，它的出现打破了RNN 和 LSTM在NLP中的各种记录，VIT这篇论文是首次将 NLP中的方法应用到了CV领域。</p>
<p>对于<strong>Transfomers</strong> 主要要了解的就是 self-attention  和 mutil-head attention的计算过程。</p>
<h2 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h2><p>我们都知道NLP中的输入是每一个词经过embeding之后得到的一个词向量的形式。</p>
<p>在Transfomer中，对于这个输入有一个Q，K，V矩阵，分别用于和输入进行矩阵乘法，得到 q k v（不太明白是什么，但是不重要）</p>
<p><img src="/../../images/image-20221128175049144.png" alt="image-20221128175049144"></p>
<ul>
<li>对于每一个输入词都会经过这个 Q K V矩阵，得到不同的  q k v矩阵，每个**q **都会和 <strong>k</strong> 两两进行点乘运算，所以每个词向量都会得到和输入词个数相同的输出 α。</li>
<li>每个词向量输出再经过<strong>soft-max</strong> ，得到每个输入词的一个概率值。这个概率值其实是一个权重，用于表示更会注意到哪个v</li>
</ul>
<p><img src="/../../images/image-20221128175645555.png" alt="image-20221128175645555"></p>
<ul>
<li>利用上面的得到的 结果 ，每一个 α 和刚刚用于计算的k相同序号的词向量对应的v进行一次矩阵乘法。v可以理解成由输入a中提取得到的特征</li>
<li>每个α都再次进行相加得到输出 b，相当于一个词向量的输入就会有一个输出。并且维度是不会发生改变的。</li>
</ul>
<p><img src="/../../images/image-20221128180422625.png" alt="image-20221128180422625"></p>
<h2 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h2><p>多头自注意力就是将，q、k、v分成多个部分分别计算。</p>
<p>如：现在对于一个词向量的 q、k、v 是 1 X 4 的，如果有两个头，就会均分这个q、k、v变成两个1 X 2的部分，每个词向量对应的 q、k、v都是两个1X2的，假设每一个词向量的分头都有标号（1-n，例子中n&#x3D;2），把所有词向量标号相同的 q、k、v合在一起作为一个 head，每个head 都会去做 self-attention，这时候每个头的q、k、v输出就是1X2的，又分成两个头，最后面将这两个头拼起来，还是变成了一个1 X 4的输出。</p>
<h1 id="VIT"><a href="#VIT" class="headerlink" title="VIT"></a>VIT</h1><p>vit是transformer在视觉领域的应用，由于对于Transfomer的输入是一个个1D的token，而图像是一个2D的矩阵。</p>
<p>所以文章就对图像做了一个<strong>切分</strong>，将一个2D的图片转为1D的token，直接作为Tranfromer的输入。</p>
<p>这里<strong>切分</strong>的做法就是，将一张图片分成大小16X16的小块， 切分出来的个数就是作为Transformer的输入的token的个数，文章将这个大小称为patch size。</p>
<p>然后将每个patch经过要给线性投射层，将16 X 16 X 3的patch映射为一个 1 X 768 的 1D的token，然后再借鉴NLP处理的中位置标签的做法，在每一个token中 <strong>sum</strong> 上一个位置编码，这个位置编码是可学习的也是一个 1 X 768的token，同样借鉴 NLP 中 的Bert的做法，在添加上一个全局的token，这个token 就是最后用来作分类使用的，最后就每个token就作为Transformer的输入。 </p>
<p><img src="/../../images/image-20221130102930732.png" alt="image-20221130102930732"></p>
<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>上图是 VIT的一个完整的结构，这里假设输入的 img 的大小为 224  X  224,patch的大小是 16 X 16。</p>
<ul>
<li><p>首先将这个图片进行切分，224 X 224 切分成大小为16 X 16的patch，这里可以直接用用一个16 X 16 X 768，Stride为 16 的卷积直接完成，输入时 224 X 224 X 3 经过这个卷积之后 得到 14 X 14 X 768，然后将宽高进行展平，得到196 X 768 的token，196就是patch的个数。</p>
</li>
<li><p>每一个token还需要一个位置编码，这里的位置编码时通过<strong>sum</strong> 完成的 而不是 <strong>concat</strong> ，所以加上位置编码后token 还是 196 X 768，最后还有一个全局的分类token，这个就是concat，最后输入的token就是 197 X 768。</p>
</li>
<li><p>准备好token 之后就是进入 Transformer encoder，这个encoder会循环 L 次，</p>
<ul>
<li>首先token会经过 一个layer normal，输出依然时 197 X 768，layer normal 实际上就是对一个token做了一次归一化。</li>
<li>计算Q、K、V， 是通过一个全连接来实现的，并且共享权重，输入是197X768 经过全连接后得到 197X2304，前768就是Q、第二个768就是K、第三个就是V</li>
<li>然后经过一个多头自注意力，这里以12头为例，token经过q、k、v矩阵得到每个都为 197 X 768的q、k、v，然后将这写q、k、v分成12份，每份就是197 X 64。</li>
<li>每一份分别做自注意力，每一份的输出都是197 X 64，再将每一份合并输出还是197 X 768，然后会做一次类似于残差的操作，将输入和输出进行相加，送入layer normal + MLP</li>
<li>经过layer normal 之后，会进入MLP ，MLP会先将输入的维度放大的输入的4倍，变成197 X 3072，再将197 X 3027 调整到197 X 768，做残差。</li>
<li>循环 L次</li>
</ul>
</li>
<li><p>利用全局的分类token，做输出。</p>
</li>
</ul>
<h1 id="DETR"><a href="#DETR" class="headerlink" title="DETR"></a>DETR</h1><p>文章一开始指出现在的目标检测基本上都是基于在一大堆的proposal、anchor或者以bbox为中心上的回归和分类的问题。</p>
<p>这些做法和设计确实取得了很好的结果，但是这些做法受到后处理操作和先验人工设置的anchor的严重影响。并且这些操作并不是所有机器都支持加速计算。</p>
<p>为此文章提出一种基于Trannsformer的一种完全端到端的目标检测的算法。把目标检测当成是一个集合预测的问题，通过二分图匹配来代替传统做法中的anchor和nms机制。</p>
<p>DETR的整个结构可以分为四部分：</p>
<ul>
<li>CNN提取全局特征</li>
<li>Transformer编码器添加注意力机制</li>
<li>Transformer解码器，二分图匹配为每一个GT选择一个最好的预测特征，设定每一个图只会给出100个预测feature map</li>
<li>得到feature map 进行分类和回归</li>
</ul>
<p><img src="/../../images/image-20221202153303983.png" alt="image-20221202153303983"></p>
<p><img src="/../../images/image-20221202155049489.png" alt="image-20221202155049489"></p>
<p>解码器前面的部分就是一个很常规的混合型 transformer的一个结构，通过CNN提取全局特征，然后加上位置编码，将feature map 每个通道当作一个 patch，展平长宽输入transformer编码器。</p>
<ul>
<li>假设输入图像是 3 X 800 X1088的图像，经过cnn之后长宽变为原来的 1&#x2F;32，通道数 调整为256</li>
<li>256 X 25 X 34的feature map，将长宽展平，加上位置编码，得到850 X 256的token，作为transformer编码器的输入，transformer编码器重复6次得到的还是 850 X 256</li>
</ul>
<p>解码器和检测头部分：</p>
<p>首先会固定detr的输出个数为100个框，那这些框要不使用nms，怎么将框分配给哪个gt呢？</p>
<p>文章中将这个问题转换为了一个<strong>二分图匹配</strong>的问题，什么是二分图匹配？</p>
<p>假设我现在用5个工人，每个工人可以独立完成一个工作，但是开销各不相同，一共有3个工作。那么将这五个工人和3个工作的开销写成一个矩阵的形式就能够得到一个 开销举证。</p>
<p><img src="/../../images/image-20221202162348731.png" alt="image-20221202162348731"></p>
<p>然后根据这个矩阵，我们通过遍历的算法找到一个唯一解使得完成这些工作的开销最低，但是显然逐个遍历时间复杂度就会很高，所以文章说可以使用别人提出的一个 算法 <strong>匈牙利算法</strong> 来得到这么一个最优解。</p>
<p>回到detr的输出部分，我们把预测得到的100个框当作工人，GT当作工作，矩阵中的内容表示当把这个预测框分配给这个GT时的损失是多少也就是这个框的分类和损失之和，然后使用匈牙利算法为每个GT选择一个最优的预测框，其余的框就被当作没有目标的框。</p>
<p>所以这里我们就知道了为什么detr不需要做后处理，因为每个GT我限制只会输出一个预测框与之对应。得到这些框之后，就可以利用一个最终的目标函数来计算损失总和，从而完成损失回传。</p>
<ul>
<li>解码器部分</li>
</ul>
<p>解码器部分就是输出那100个预测的feature map的，首先先固定了一个 object query 它的shape 就是 100 X 256，100 就是为了固定解码器的输出是100，256则是为了和编码器部分对应做 attention 计算，这里其实就是相当于把 object query当作 attention的 q矩阵，而编码器的输出就是 k 和 v矩阵，然后根据 transformer的计算规则分别进行计算。</p>
<p>这里的object query的初始值文中设定的就是 全0的一个tensor 加上 位置编码信息，并且编码器相比于nlp中的transformer是并行输出的，直接得到100 X 256的矩阵，而不是每次得到一个 1X 256的向量。</p>
<ul>
<li>检测头</li>
</ul>
<p>检测头和传统的检测头几乎一样，只不过编码器得到100 X 256的结果之后会先经过二分图匹配，得到最优解之后再对结果进行分类和回归计算损失。</p>
<ul>
<li>问题</li>
</ul>
<p>DETR 收敛速度慢，并且输入的尺寸要求是较小的，因为要用到transformer，序列肯定不能太长。又因为输入的尺寸是较小的，这也就导致了对于小目标的检测效果不是很好。</p>
<h1 id="DEFORMABLE-DETR"><a href="#DEFORMABLE-DETR" class="headerlink" title="DEFORMABLE DETR"></a>DEFORMABLE DETR</h1><p>这篇文章，是基于传统DETR的问题进行改进的一个模型。传统的DETR由于transformer的限制，导致输入序列不能太长。这篇文章认为序列太长要被训练的时间特别长，是因为，在计算的时候transformer两两之间都进行了计算。 这篇文章提出没必要两两之间进行计算注意力，因为有些距离很远的计算了也是浪费计算资源。所以只要计算附近的一些进行计算就好了。</p>
<p>大致的过程：</p>
<p>RestNet50提取出特征，一共4层。假设每一层的特征的维度如下：</p>
<p>L1：1，256，54，48</p>
<p>L2：1，256，27，24</p>
<p>L3：1，256，14，12</p>
<p>L4：1，256，7，6</p>
<p>这些就是要输入 transformer encode的token，每一层都展平，得到：</p>
<p>L1：1，256，2592</p>
<p>L2：1，256，648</p>
<p>L3：1，256，168</p>
<p>L4：1，256，46</p>
<p>Cat 一下，target：1，256，3454。这个就是encoder的输入。</p>
<p>初始化论文中的reference point，这也是可变形中最重要的部分。对于一层来说，这个reference point 其实可以看作是一个二维的位置编码。54，48的  h，w来说，每一个reference point 就是 (1,1)、(1,2)….(54,48)。</p>
<p>但是对于多维的来说，这样的绝对编码就有问题了。第一层最大的point在最小的那一层已经越界了。所以要用相对位置来标识，每层中的相对位置都是0-1的。</p>
<p>第一层就是 (0.018,0.02)….，这样每一层在其他层中一定可以找到对应的位置。这只是初始化。相当于对于一开始，每个点只关注到自己这个点。</p>
<p>然后我们要通过一个FFN得到每个点的偏移量，相当于对于每个点，应该更关注哪一个点的。这个FFN的输入用的是上面构建的 encoder输入的 token序列。也就是用特征得到的一个偏移量的值。然后将偏移量加上初始化的reference point就能得到每个reference 应该关注的点。这也是偏移量对齐的一个操作</p>
<p>这里的偏移量为了能够和reference point 相加，所以只有x轴和y轴<strong>2</strong>个方向。一共预测<strong>4</strong>个方向，也就是上下左右，这是一个层级中的每个token序列要预测得到的偏移量。一共有<strong>4</strong>个层级，然后又是做的多头的注意力操作，源码中是 <strong>8</strong>个头。所以一共要预测的偏移量的个数是 <strong>2X4X4X8&#x3D;256</strong> 所以 FFN的输出就是256，这里的256是上面这样算来的而不是为了保证输入输出一致。</p>
<p>得到偏移量之后，就要去计算attention 了，传统transformer是通过，Q和K计算得到attention的，但是这里它又是直接又token序列得到的attention，由token经过一个FFC，输出是 8X4X4，和上面的解释是一样的，但是这里我们只需要知道关注到哪个方向的点上就可以了，也就是每个点的权重大小。</p>
<p>除了上面这些东西之外，token序列还经过一个普通的FFC得到了 V矩阵。</p>
<p>到这里我们来看输入的 由Resnet50得到的4层特征拼接起来得到的特征序列生成了哪些东西。</p>
<ul>
<li>每个token对应每个head的每一层的每个点的X、Y坐标的偏移量</li>
<li>V矩阵</li>
<li>每个head的每一层的每个点的权重</li>
</ul>
<p>有了上面这些东西，我们遍历每一层，然后根据每一层取出这一层的偏移量、V矩阵、权重。</p>
<p>然后再每一层中用偏移量矩阵对V矩阵进行采样，用采样点周围四个点的值做双线性插值。将特征用偏移量对齐，然后乘以attention的权重得到encoder最后的特征输出 记为 memory。</p>
<p>得到encoder输出之后，就和detr一样，有一个object query，detr中是100个，这里初始化了300个，这个object query会先加上自己的位置编码，然后和自己做一次</p>
<p>self-attention，然后被当作decoder的 q矩阵，v矩阵就是 memory。然后就是多次的，用q生成偏移，v和偏移对齐，q得到权重，v乘以权重，得到最后的特征。用最后的特征取 做cls 和 reg。</p>
<h2 id="源码部分"><a href="#源码部分" class="headerlink" title="源码部分"></a>源码部分</h2><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>这里用了多尺度的输入，所以有两种位置编码。对于每个尺度的特征图会有一个正余弦的位置编码，对于每一层还有一个单独的位置编码为的是区分不同层相同位置上的位置编码。</p>
<h3 id="reference-point-生成"><a href="#reference-point-生成" class="headerlink" title="reference point 生成"></a>reference point 生成</h3><p><img src="/../../images/image-20230904230700878.png" alt="image-20230904230700878"></p>
<p>reference_points ： 相对位置编码的reference point，并且拼接拉长了的。因为要对应到 输入transformer的token序列的长度，所以这里的序列维度是**[bs,输入序列长度,ch]** 。</p>
<p>输入序列的长度：四层多尺度的  h*w 拼接在一起的结果。</p>
<p>假设我们的四层特征图如下：</p>
<p><img src="/../../images/image-20230904232049720.png" alt="image-20230904232049720"></p>
<p>根据上面这个输入序列，每一个点的计算过程如下：</p>
<p><img src="/../../images/image-20230904233246372.png" alt="image-20230904233246372"></p>
<p>依次取得不同层的每个reference point 然后计算特征加权和。</p>
<p>明确了计算流程，这一行代码就很好理解了</p>
<p><img src="/../../images/image-20230905200550893.png" alt="image-20230905200550893"></p>
<p>我们现在只有每一层的reference points 维度是 [b,token长度,2]</p>
<p>为了方便在每一层取得reference point ，我们要把每一层的reference point进行复制。</p>
<p> reference_points[:, :, None] 这个会给 reference_points 的第三维添加一个维度，这样就变成了 [b,token长度,1,2]</p>
<p> valid_ratios 的维度是 [b,layer_num,2]，每一层的有效的比例，</p>
<p> valid_ratios[:, None] 会在第二维添加上一个新的维度，[b,1,layer_num,2]</p>
<p>根据tensor的计算规则，会得到一个 [b,token长度,layer_num,2]的tensor，最后两个维度就是对每一层 reference points 的 (x,y) 复制一次，方便后面计算(x,y)在不同层的对应的值。这就生成了reference points。</p>
<h3 id="encoder部分"><a href="#encoder部分" class="headerlink" title="encoder部分"></a>encoder部分</h3><p><img src="/../../images/image-20230905203401694.png" alt="image-20230905203401694"></p>
<p>整个encoder的内容都是有 输入特征得到的，所以输入的  k,v为None。</p>
<p>整个encoder部分同样是分为四部分：self_atten，norm，ffn，norm。</p>
<ul>
<li>self_atten</li>
</ul>
<p><img src="/../../images/image-20230905204449033.png" alt="image-20230905204449033"></p>
<p>这里传进去的都是 q，在self_atten里面，会先将q加上上面准备好的位置编码。对v会先经过一个ffn 但是 通道数不变，可以理解为V也是有q经过一个ffc得到的。</p>
<p><img src="/../../images/image-20230905210324971.png" alt="image-20230905210324971"></p>
<p>然后就是一直使用q获得要用的东西。</p>
<p><img src="/../../images/image-20230905210427211.png" alt="image-20230905210427211"></p>
<p>sampling_offsets： 相当于根据输入的特征生成了对于reference points的偏移量，这里的输出通道数是：reference points用x，y坐标表示所以<strong>2</strong>个值。一共预测<strong>4</strong>个方向，也就是上下左右，这是一个层级中的每个token序列要预测得到的偏移量。一共有<strong>4</strong>个层级，然后又是做的多头的注意力操作，源码中是 <strong>8</strong>个头。所以一共要预测的偏移量的个数是 <strong>2X4X4X8&#x3D;256</strong> 所以 这里FFN的输出就是256。</p>
<p>可以这样去理解，这里做了多头，每个头64的维度，一共8个头，8个头的每一个特征都要在4层多尺度中，预测出不同层的4个方向的偏移，每个偏移量的  x，y大小。这里的结果是绝对偏移量。</p>
<p>attention_weights： 和上面类似，每个头每层四个方向的偏移量的权重大小。同样是由 query向量，经过ffc得到的。这里有点类似于传统transformer的 q和k做计算，然后经过softmax 得到 注意力权重大小。</p>
<p>得到了偏移量之后，就可以将他和reference point做对齐了。</p>
<p><img src="/../../images/image-20230905235309247.png" alt="image-20230905235309247"></p>
<p>上面也说到了 ffc得到的 偏移量是绝对偏移量，还要在每一层上做归一化，然后和reference points 做对齐。</p>
<p>reference point 生成的时候，在每个层上复制了四次，就是为了这边方便加起来。最终会得到的维度是：[bs,token长度,8,4,4,2]</p>
<p>表示的就是每个头的每个reference point 在每一层的四个采样点。</p>
<p>得到具体的采样点之后，就要对特征进行采样了。</p>
<p>特征就是，由输入特征经过ffc得到的 value，我们就要用采样点在这个上面进行采样，然后乘以每个attention 的权重。</p>
<p>会根据每一层的采样的位置，在每一层特征上进行采样，一个特征会对应4个采样，然后新的特征就是这个采样经过双线性插值得到的。并且采样的位置肯定不是一个整数，所以采样点的特征也会用采样点周围的四个点的做双线性插值得到。</p>
<p>这样就完成了特征采样，其实就是一个对齐的操作。然后就根据每层的权重做一个加权和就得到一次MultiScaleDeformableAttn的结果，在做一个残差就得到一次self_atten的结果。再接上norm和ffc就是一个完整的encode的过程，一共做6次。</p>
<h3 id="decoder部分"><a href="#decoder部分" class="headerlink" title="decoder部分"></a>decoder部分</h3><p>上面encoder部分可以当作就是要得到一个较好的特征，所以里面一直都是拿输入序列一直做atten操作。得到encoder的输出结果。</p>
<p>decoder部分就是要用到encoder的结果，来得到目标的。</p>
<p>传统DETR decoder部分初始化的 目标是 100个，这里做了多尺度，所以增加到了300个。</p>
<p>decoer一开始先给这个初始的 300个query做了一个self_atten，并且decoder也是要做deformable的，也要做reference point。</p>
<p>decoder做完self_atten之后，得到的结果就是decoder的query，而encoder的输出就是decoder的value，就用query和value做计算。</p>
<p>300个query，ffc得到偏移量、权重。 偏移量在value里拿到对应的特征，做加权和，最后面就是重构这300个query向量。</p>
<p>然后就可以拿着这300个重构后的query向量去做分类和回归得到300个框的类别和bbox。</p>
<p>最后将这300个结果和gt做二分图匹配，有几个gt就会得到几个最终的预测结果。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://yimeng436.github.io">异梦</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://yimeng436.github.io/2022/08/21/Paper-CV_Transformer%E7%B3%BB%E5%88%97/Paper-CV_Transformer%E7%B3%BB%E5%88%97/">https://yimeng436.github.io/2022/08/21/Paper-CV_Transformer%E7%B3%BB%E5%88%97/Paper-CV_Transformer%E7%B3%BB%E5%88%97/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yimeng436.github.io" target="_blank">异梦的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="/img/top.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/08/22/Project-OJ/Project-OJ/"><img class="prev-cover" src="/img/top.png" onerror="onerror=null;src='/img/cover.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Project</div></div></a></div><div class="next-post pull-right"><a href="/2022/08/21/NewCod_Java/NewCod_Java/"><img class="next-cover" src="/img/top.png" onerror="onerror=null;src='/img/cover.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NewCode_Java</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/06/06/MNIST/MNIST/" title="MNIST"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">MNIST</div></div></a></div><div><a href="/2022/06/06/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD/" title="前向传播"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">前向传播</div></div></a></div><div><a href="/2022/06/06/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="卷积神经网络"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">卷积神经网络</div></div></a></div><div><a href="/2022/06/06/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/" title="梯度下降算法"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">梯度下降算法</div></div></a></div><div><a href="/2022/06/06/tensorflow/tensorflow/" title="tensorflow"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">tensorflow</div></div></a></div><div><a href="/2022/06/15/PyTorch/PyTorch/" title="PyTorch"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-15</div><div class="title">PyTorch</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="http://mms2.baidu.com/it/u=665033858,976373917&amp;fm=253&amp;app=138&amp;f=JPEG&amp;fmt=auto&amp;q=75?w=500&amp;h=500" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">异梦</div><div class="author-info__description">欢迎访问</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">62</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/yimeng436" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2441844062@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transfomers"><span class="toc-number">1.</span> <span class="toc-text">Transfomers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#self-attention"><span class="toc-number">1.1.</span> <span class="toc-text">self-attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-head-attention"><span class="toc-number">1.2.</span> <span class="toc-text">Multi-head attention</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#VIT"><span class="toc-number">2.</span> <span class="toc-text">VIT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">2.1.</span> <span class="toc-text">前向传播</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DETR"><span class="toc-number">3.</span> <span class="toc-text">DETR</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DEFORMABLE-DETR"><span class="toc-number">4.</span> <span class="toc-text">DEFORMABLE DETR</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E9%83%A8%E5%88%86"><span class="toc-number">4.1.</span> <span class="toc-text">源码部分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">4.1.1.</span> <span class="toc-text">位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reference-point-%E7%94%9F%E6%88%90"><span class="toc-number">4.1.2.</span> <span class="toc-text">reference point 生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder%E9%83%A8%E5%88%86"><span class="toc-number">4.1.3.</span> <span class="toc-text">encoder部分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decoder%E9%83%A8%E5%88%86"><span class="toc-number">4.1.4.</span> <span class="toc-text">decoder部分</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/08/19/MySQL%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86/MySQL%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86/" title="MySQL原理"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="MySQL原理"/></a><div class="content"><a class="title" href="/2023/08/19/MySQL%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86/MySQL%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86/" title="MySQL原理">MySQL原理</a><time datetime="2023-08-18T16:00:00.000Z" title="发表于 2023-08-19 00:00:00">2023-08-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/19/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/" title="算法笔记"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="算法笔记"/></a><div class="content"><a class="title" href="/2023/08/19/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/" title="算法笔记">算法笔记</a><time datetime="2023-08-18T16:00:00.000Z" title="发表于 2023-08-19 00:00:00">2023-08-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/06/%E5%89%8D%E7%AB%AF%E7%9F%A5%E8%AF%86/%E5%89%8D%E7%AB%AF%E7%9F%A5%E8%AF%86/" title="前端知识"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="前端知识"/></a><div class="content"><a class="title" href="/2023/06/06/%E5%89%8D%E7%AB%AF%E7%9F%A5%E8%AF%86/%E5%89%8D%E7%AB%AF%E7%9F%A5%E8%AF%86/" title="前端知识">前端知识</a><time datetime="2023-06-05T16:00:00.000Z" title="发表于 2023-06-06 00:00:00">2023-06-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/14/Paper-FSAF/Paper-FSAF/" title="FSAF论文"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="FSAF论文"/></a><div class="content"><a class="title" href="/2022/10/14/Paper-FSAF/Paper-FSAF/" title="FSAF论文">FSAF论文</a><time datetime="2022-10-13T16:00:00.000Z" title="发表于 2022-10-14 00:00:00">2022-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/25/Paper-Faster-RCNN/Paper-Faster-RCNN/" title="Faster-RCNN论文"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="Faster-RCNN论文"/></a><div class="content"><a class="title" href="/2022/08/25/Paper-Faster-RCNN/Paper-Faster-RCNN/" title="Faster-RCNN论文">Faster-RCNN论文</a><time datetime="2022-08-24T16:00:00.000Z" title="发表于 2022-08-25 00:00:00">2022-08-25</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 异梦</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>