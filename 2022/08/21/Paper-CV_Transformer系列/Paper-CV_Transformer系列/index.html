<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Paper-CV_Transformer系列 | 异梦的博客</title><meta name="keywords" content="深度学习"><meta name="author" content="异梦"><meta name="copyright" content="异梦"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="VIT">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper-CV_Transformer系列">
<meta property="og:url" content="https://yimeng436.github.io/2022/08/21/Paper-CV_Transformer%E7%B3%BB%E5%88%97/Paper-CV_Transformer%E7%B3%BB%E5%88%97/index.html">
<meta property="og:site_name" content="异梦的博客">
<meta property="og:description" content="VIT">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yimeng436.github.io/img/top.png">
<meta property="article:published_time" content="2022-08-20T16:00:00.000Z">
<meta property="article:modified_time" content="2022-12-06T01:28:22.009Z">
<meta property="article:author" content="异梦">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yimeng436.github.io/img/top.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://yimeng436.github.io/2022/08/21/Paper-CV_Transformer%E7%B3%BB%E5%88%97/Paper-CV_Transformer%E7%B3%BB%E5%88%97/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Paper-CV_Transformer系列',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-12-06 09:28:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="http://mms2.baidu.com/it/u=665033858,976373917&amp;fm=253&amp;app=138&amp;f=JPEG&amp;fmt=auto&amp;q=75?w=500&amp;h=500" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">46</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/top.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">异梦的博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Paper-CV_Transformer系列</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-08-20T16:00:00.000Z" title="发表于 2022-08-21 00:00:00">2022-08-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-06T01:28:22.009Z" title="更新于 2022-12-06 09:28:22">2022-12-06</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Paper-CV_Transformer系列"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><strong><center><font size=5>CV_Transformer系列</font></center></strong></p>
<h1 id="Transfomers"><a href="#Transfomers" class="headerlink" title="Transfomers"></a>Transfomers</h1><p>Transfomers 本身是<strong>NLP</strong> 中的一个方法，它的出现打破了RNN 和 LSTM在NLP中的各种记录，VIT这篇论文是首次将 NLP中的方法应用到了CV领域。</p>
<p>对于<strong>Transfomers</strong> 主要要了解的就是 self-attention  和 mutil-head attention的计算过程。</p>
<h2 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h2><p>我们都知道NLP中的输入是每一个词经过embeding之后得到的一个词向量的形式。</p>
<p>在Transfomer中，对于这个输入有一个Q，K，V矩阵，分别用于和输入进行矩阵乘法，得到 q k v（不太明白是什么，但是不重要）</p>
<p><img src="/../../images/image-20221128175049144.png" alt="image-20221128175049144"></p>
<ul>
<li>对于每一个输入词都会经过这个 Q K V矩阵，得到不同的  q k v矩阵，每个**q **都会和 <strong>k</strong> 两两进行点乘运算，所以每个词向量都会得到和输入词个数相同的输出 α。</li>
<li>每个词向量输出再经过<strong>soft-max</strong> ，得到每个输入词的一个概率值</li>
</ul>
<p><img src="/../../images/image-20221128175645555.png" alt="image-20221128175645555"></p>
<ul>
<li>利用上面的得到的 结果 ，每一个 α 和刚刚用于计算的k相同序号的词向量对应的v进行一次矩阵乘法</li>
<li>每个α都再次进行相加得到输出 b，相当于一个词向量的输入就会有一个输出</li>
</ul>
<p><img src="/../../images/image-20221128180422625.png" alt="image-20221128180422625"></p>
<h2 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h2><p>多头自注意力就是将，q、k、v分成多个部分分别计算。</p>
<p>如：现在对于一个词向量的 q、k、v 是 1 X 4 的，如果有两个头，就会均分这个q、k、v变成两个1 X 2的部分，每个词向量对应的 q、k、v都是两个1X2的，假设每一个词向量的分头都有标号（1-n），所有标号相同的 q、k、v合在一起作为一个 head，每个head 都会去做 self-attention，这时候每个头的q、k、v输出就是1X2的，又分成两个头，最后面将这两个头拼起来，还是变成了一个1 X 4的输出。</p>
<h1 id="VIT"><a href="#VIT" class="headerlink" title="VIT"></a>VIT</h1><p>vit是transformer在视觉领域的应用，由于对于Transfomer的输入是一个个1D的token，而图像是一个2D的矩阵。</p>
<p>所以文章就对图像做了一个<strong>切分</strong>，将一个2D的图片转为1D的token，直接作为Tranfromer的输入。</p>
<p>这里<strong>切分</strong>的做法就是，将一张图片分成大小16X16的小块， 切分出来的个数就是作为Transformer的输入的token的个数，文章将这个大小称为patch size。</p>
<p>然后将每个patch经过要给线性投射层，将16 X 16 X 3的patch映射为一个 1 X 768 的 1D的token，然后再借鉴NLP处理的中位置标签的做法，在每一个token中 <strong>sum</strong> 上一个位置编码，这个位置编码是可学习的也是一个 1 X 768的token，同样借鉴 NLP 中 的Bert的做法，在添加上一个全局的token，这个token 就是最后用来作分类使用的，最后就每个token就作为Transformer的输入。 </p>
<p><img src="/../../images/image-20221130102930732.png" alt="image-20221130102930732"></p>
<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>上图是 VIT的一个完整的结构，这里假设输入的 img 的大小为 224  X  224,patch的大小是 16 X 16。</p>
<ul>
<li><p>首先将这个图片进行切分，224 X 224 切分成大小为16 X 16的patch，这里可以直接用用一个16 X 16 X 768，Stride为 16 的卷积直接完成，输入时 224 X 224 X 3 经过这个卷积之后 得到 14 X 14 X 768，然后将宽高进行展平，得到196 X 768 的token，196就是patch的个数。</p>
</li>
<li><p>每一个token还需要一个位置编码，这里的位置编码时通过<strong>sum</strong> 完成的 而不是 <strong>concat</strong> ，所以加上位置编码后token 还是 196 X 768，最后还有一个全局的分类token，这个就是concat，最后输入的token就是 197 X 768。</p>
</li>
<li><p>准备好token 之后就是进入 Transformer encoder，这个encoder会循环 L 次，</p>
<ul>
<li>首先token会经过 一个layer normal，输出依然时 197 X 768，layer normal 实际上就是对一个token做了一次归一化。</li>
<li>然后经过一个多头自注意力，这里以12头为例，token经过q、k、v矩阵得到每个都为 197 X 768的q、k、v，然后将这写q、k、v分成12份，每份就是197 X 64。</li>
<li>每一份分别做自注意力，每一份的输出都是197 X 64，再将每一份合并输出还是197 X 768，然后会做一次类似于残差的操作，将输入和输出进行相加，送入layer normal + MLP</li>
<li>经过layer normal 之后，会进入MLP ，MLP会先将输入的维度放大的输入的4倍，变成197 X 3072，再将197 X 3027 调整到197 X 768，做残差。</li>
<li>循环 L次</li>
</ul>
</li>
<li><p>利用全局的分类token，做输出。</p>
</li>
</ul>
<h1 id="DETR"><a href="#DETR" class="headerlink" title="DETR"></a>DETR</h1><p>文章一开始指出现在的目标检测基本上都是基于在一大堆的proposal、anchor或者以bbox为中心上的回归和分类的问题。</p>
<p>这些做法和设计确实取得了很好的结果，但是这些做法受到后处理操作和先验人工设置的anchor的严重影响。并且这些操作并不是所有机器都支持加速计算。</p>
<p>为此文章提出一种基于Trannsformer的一种完全端到端的目标检测的算法。把目标检测当成是一个集合预测的问题，通过二分图匹配来代替传统做法中的anchor和nms机制。</p>
<p>DETR的整个结构可以分为四部分：</p>
<ul>
<li>CNN提取全局特征</li>
<li>Transformer编码器添加注意力机制</li>
<li>Transformer解码器，二分图匹配为每一个GT选择一个最好的预测特征，设定每一个图只会给出100个预测feature map</li>
<li>得到feature map 进行分类和回归</li>
</ul>
<p><img src="/../../images/image-20221202153303983.png" alt="image-20221202153303983"></p>
<p><img src="/../../images/image-20221202155049489.png" alt="image-20221202155049489"></p>
<p>解码器前面的部分就是一个很常规的混合型 transformer的一个结构，通过CNN提取全局特征，然后加上位置编码，将feature map 每个通道当作一个 patch，展平长宽输入transformer编码器。</p>
<ul>
<li>假设输入图像是 3 X 800 X1088的图像，经过cnn之后长宽变为原来的 1&#x2F;32，通道数 调整为256</li>
<li>256 X 25 X 34的feature map，将长宽展平，加上位置编码，得到850 X 256的token，作为transformer编码器的输入，transformer编码器重复6次得到的还是 850 X 256</li>
</ul>
<p>解码器和检测头部分：</p>
<p>首先会固定detr的输出个数为100个框，那这些框要不使用nms，怎么将框分配给哪个gt呢？</p>
<p>文章中将这个问题转换为了一个<strong>二分图匹配</strong>的问题，什么是二分图匹配？</p>
<p>假设我现在用5个工人，每个工人可以独立完成一个工作，但是开销各不相同，一共有3个工作。那么将这五个工人和3个工作的开销写成一个矩阵的形式就能够得到一个 开销举证。</p>
<p><img src="/../../images/image-20221202162348731.png" alt="image-20221202162348731"></p>
<p>然后根据这个矩阵，我们通过遍历的算法找到一个唯一解使得完成这些工作的开销最低，但是显然逐个遍历时间复杂度就会很高，所以文章说可以使用别人提出的一个 算法 <strong>匈牙利算法</strong> 来得到这么一个最优解。</p>
<p>回到detr的输出部分，我们把预测得到的100个框当作工人，GT当作工作，矩阵中的内容表示当把这个预测框分配给这个GT时的损失是多少也就是这个框的分类和损失之和，然后使用匈牙利算法为每个GT选择一个最优的预测框，其余的框就被当作没有目标的框。</p>
<p>所以这里我们就知道了为什么detr不需要做后处理，因为每个GT我限制只会输出一个预测框与之对应。得到这些框之后，就可以利用一个最终的目标函数来计算损失总和，从而完成损失回传。</p>
<ul>
<li>解码器部分</li>
</ul>
<p>解码器部分就是输出那100个预测的feature map的，首先先固定了一个 object query 它的shape 就是 100 X 256，100 就是为了固定解码器的输出是100，256则是为了和编码器部分对应做 attention 计算，这里其实就是相当于把 object query当作 attention的 q矩阵，而编码器的输出就是 k 和 v矩阵，然后根据 transformer的计算规则分别进行计算。</p>
<p>这里的object query的初始值文中设定的就是 全0的一个tensor 加上 位置编码信息，并且编码器相比于nlp中的transformer是并行输出的，直接得到100 X 256的矩阵，而不是每次得到一个 1X 256的向量。</p>
<ul>
<li>检测头</li>
</ul>
<p>检测头和传统的检测头几乎一样，只不过编码器得到100 X 256的结果之后会先经过二分图匹配，得到最优解之后再对结果进行分类和回归计算损失。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://yimeng436.github.io">异梦</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://yimeng436.github.io/2022/08/21/Paper-CV_Transformer%E7%B3%BB%E5%88%97/Paper-CV_Transformer%E7%B3%BB%E5%88%97/">https://yimeng436.github.io/2022/08/21/Paper-CV_Transformer%E7%B3%BB%E5%88%97/Paper-CV_Transformer%E7%B3%BB%E5%88%97/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yimeng436.github.io" target="_blank">异梦的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="/img/top.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/08/22/Project/Project/"><img class="prev-cover" src="/img/top.png" onerror="onerror=null;src='/img/cover.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Project</div></div></a></div><div class="next-post pull-right"><a href="/2022/08/21/NewCod_Java/NewCod_Java/"><img class="next-cover" src="/img/top.png" onerror="onerror=null;src='/img/cover.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NewCode_Java</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/06/06/MNIST/MNIST/" title="MNIST"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">MNIST</div></div></a></div><div><a href="/2022/06/06/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD/" title="前向传播"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">前向传播</div></div></a></div><div><a href="/2022/06/06/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="卷积神经网络"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">卷积神经网络</div></div></a></div><div><a href="/2022/06/06/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/" title="梯度下降算法"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">梯度下降算法</div></div></a></div><div><a href="/2022/06/06/tensorflow/tensorflow/" title="tensorflow"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">tensorflow</div></div></a></div><div><a href="/2022/06/15/PyTorch/PyTorch/" title="PyTorch"><img class="cover" src="/img/top.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-15</div><div class="title">PyTorch</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="http://mms2.baidu.com/it/u=665033858,976373917&amp;fm=253&amp;app=138&amp;f=JPEG&amp;fmt=auto&amp;q=75?w=500&amp;h=500" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">异梦</div><div class="author-info__description">欢迎访问</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">46</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/yimeng436" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2441844062@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transfomers"><span class="toc-number">1.</span> <span class="toc-text">Transfomers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#self-attention"><span class="toc-number">1.1.</span> <span class="toc-text">self-attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-head-attention"><span class="toc-number">1.2.</span> <span class="toc-text">Multi-head attention</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#VIT"><span class="toc-number">2.</span> <span class="toc-text">VIT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">2.1.</span> <span class="toc-text">前向传播</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DETR"><span class="toc-number">3.</span> <span class="toc-text">DETR</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/10/14/Paper-FSAF/Paper-FSAF/" title="FSAF论文"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="FSAF论文"/></a><div class="content"><a class="title" href="/2022/10/14/Paper-FSAF/Paper-FSAF/" title="FSAF论文">FSAF论文</a><time datetime="2022-10-13T16:00:00.000Z" title="发表于 2022-10-14 00:00:00">2022-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/25/Paper-Faster-RCNN/Paper-Faster-RCNN/" title="Faster-RCNN论文"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="Faster-RCNN论文"/></a><div class="content"><a class="title" href="/2022/08/25/Paper-Faster-RCNN/Paper-Faster-RCNN/" title="Faster-RCNN论文">Faster-RCNN论文</a><time datetime="2022-08-24T16:00:00.000Z" title="发表于 2022-08-25 00:00:00">2022-08-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/25/Netty/Netty/" title="Netty"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="Netty"/></a><div class="content"><a class="title" href="/2022/08/25/Netty/Netty/" title="Netty">Netty</a><time datetime="2022-08-24T16:00:00.000Z" title="发表于 2022-08-25 00:00:00">2022-08-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/23/Nginx/nginx/" title="Nginx"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="Nginx"/></a><div class="content"><a class="title" href="/2022/08/23/Nginx/nginx/" title="Nginx">Nginx</a><time datetime="2022-08-22T16:00:00.000Z" title="发表于 2022-08-23 00:00:00">2022-08-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/22/Redis/Redis/" title="Redis"><img src="/img/top.png" onerror="this.onerror=null;this.src='/img/cover.png'" alt="Redis"/></a><div class="content"><a class="title" href="/2022/08/22/Redis/Redis/" title="Redis">Redis</a><time datetime="2022-08-21T16:00:00.000Z" title="发表于 2022-08-22 00:00:00">2022-08-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 异梦</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>